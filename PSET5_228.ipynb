{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINK: https://github.com/benjaminzaidel/MSE228New/blob/master/PSET5_228.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom25% double lasso: Estimate = 3806.657, StdErr = 1114.479\n",
      "bottom25% lasso/logistic: Estimate = 3699.249, StdErr = 1059.689\n",
      "bottom25% random forest: Estimate = 4252.262, StdErr = 1111.859\n",
      "bottom25% decision tree: Estimate = 2670.163, StdErr = 1000.105\n",
      "bottom25% boosted forest: Estimate = 4147.604, StdErr = 1121.046\n",
      "top25% double lasso: Estimate = 18085.828, StdErr = 3726.298\n",
      "top25% lasso/logistic: Estimate = 18333.122, StdErr = 3711.315\n",
      "top25% random forest: Estimate = 16786.778, StdErr = 3876.613\n",
      "top25% decision tree: Estimate = 6671.882, StdErr = 3026.272\n",
      "top25% boosted forest: Estimate = 16891.658, StdErr = 3918.029\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from formulaic import Formula\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, clone\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Load dataset\n",
    "file = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv\"\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "y = data['net_tfa'].values\n",
    "D = data['e401'].values\n",
    "\n",
    "# Define low-income and high-income groups\n",
    "data_bottom = data[data['inc'] <= data['inc'].quantile(0.25)].copy()\n",
    "data_top = data[data['inc'] >= data['inc'].quantile(0.75)].copy()\n",
    "\n",
    "# Function to extract X, D, y\n",
    "def extract_XDy(df):\n",
    "    y_ = df['net_tfa'].values\n",
    "    D_ = df['e401'].values\n",
    "    X_ = df.drop(['e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "                  'hval', 'hmort', 'hequity', 'nifa', 'net_nifa', 'net_n401', 'ira',\n",
    "                  'dum91', 'icat', 'ecat', 'zhat', 'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7',\n",
    "                  'a1', 'a2', 'a3', 'a4', 'a5'], axis=1)\n",
    "    return X_, D_, y_\n",
    "\n",
    "X_bottom, D_bottom, y_bottom = extract_XDy(data_bottom)\n",
    "X_top, D_top, y_top = extract_XDy(data_top)\n",
    "\n",
    "# Feature transformation class\n",
    "class FormulaTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, formula, array=False):\n",
    "        self.formula = formula\n",
    "        self.array = array\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        df = Formula(self.formula).get_model_matrix(X)\n",
    "        return df.values if self.array else df\n",
    "\n",
    "transformer = FormulaTransformer(\n",
    "    \"0 + poly(age, degree=6, raw=True) + poly(inc, degree=8, raw=True) \"\n",
    "    \"+ poly(educ, degree=4, raw=True) + poly(fsize, degree=2, raw=True) \"\n",
    "    \"+ male + marr + twoearn + db + pira + hown\", array=True)\n",
    "\n",
    "# DML implementation\n",
    "def dml(X, D, y, model_y, model_d, nfolds=5, classifier=False):\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=1234)\n",
    "    yhat = cross_val_predict(model_y, X, y, cv=cv, n_jobs=-1)\n",
    "    \n",
    "    if classifier:\n",
    "        Dhat = cross_val_predict(model_d, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    else:\n",
    "        Dhat = cross_val_predict(model_d, X, D, cv=cv, n_jobs=-1)\n",
    "    \n",
    "    res_y = y - yhat\n",
    "    res_D = D - Dhat\n",
    "    \n",
    "    point = np.mean(res_y * res_D) / np.mean(res_D**2)\n",
    "    epsilon = res_y - point * res_D\n",
    "    var = np.mean(epsilon**2 * res_D**2) / np.mean(res_D**2)**2\n",
    "    stderr = np.sqrt(var / len(y))\n",
    "    \n",
    "    return point, stderr\n",
    "\n",
    "# Running multiple models for both income groups\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "models = {\n",
    "    \"double lasso\": (make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv)),\n",
    "                      make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))),\n",
    "    \"lasso/logistic\": (make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv)),\n",
    "                        make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))),\n",
    "    \"random forest\": (make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001)),\n",
    "                       make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001))),\n",
    "    \"decision tree\": (make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=0.001)),\n",
    "                       make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=0.001))),\n",
    "    \"boosted forest\": (make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5)),\n",
    "                        make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5)))\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for group, (X_group, D_group, y_group) in {\"bottom25%\": (X_bottom, D_bottom, y_bottom), \"top25%\": (X_top, D_top, y_top)}.items():\n",
    "    for name, (model_y, model_d) in models.items():\n",
    "        classifier = isinstance(model_d[-1], (LogisticRegressionCV, RandomForestClassifier, GradientBoostingClassifier))\n",
    "        point, stderr = dml(X_group, D_group, y_group, model_y, model_d, nfolds=5, classifier=classifier)\n",
    "        results[f\"{group} {name}\"] = (point, stderr)\n",
    "\n",
    "# Print results\n",
    "for name, (point, stderr) in results.items():\n",
    "    print(f\"{name}: Estimate = {point:.3f}, StdErr = {stderr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom25% lasso/logistic: IRM Estimate = 4493.823, StdErr = 1019.923\n",
      "bottom25% random forest: IRM Estimate = 4647.764, StdErr = 1007.430\n",
      "bottom25% decision tree: IRM Estimate = 2914.159, StdErr = 7843.394\n",
      "bottom25% boosted forest: IRM Estimate = 4931.285, StdErr = 1382.325\n",
      "top25% lasso/logistic: IRM Estimate = 18390.212, StdErr = 3819.769\n",
      "top25% random forest: IRM Estimate = 15996.150, StdErr = 4114.711\n",
      "top25% decision tree: IRM Estimate = 42408.783, StdErr = 60272.713\n",
      "top25% boosted forest: IRM Estimate = 16973.716, StdErr = 4002.514\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, clone\n",
    "from formulaic import Formula\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings related to convergence\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Load dataset\n",
    "file = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv\"\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Define low-income and high-income groups\n",
    "data_bottom = data.query('inc <= inc.quantile(.25)').copy()\n",
    "data_top = data.query('inc >= inc.quantile(.75)').copy()\n",
    "\n",
    "def extract_XDy(df):\n",
    "    y_ = df['net_tfa'].values\n",
    "    D_ = df['e401'].values\n",
    "    X_ = df.drop(['e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "                  'hval', 'hmort', 'hequity', 'nifa', 'net_nifa', 'net_n401', 'ira',\n",
    "                  'dum91', 'icat', 'ecat', 'zhat', 'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7',\n",
    "                  'a1', 'a2', 'a3', 'a4', 'a5'], axis=1)\n",
    "    return X_, D_, y_\n",
    "\n",
    "X_bottom, D_bottom, y_bottom = extract_XDy(data_bottom)\n",
    "X_top, D_top, y_top = extract_XDy(data_top)\n",
    "\n",
    "# Feature transformation class\n",
    "class FormulaTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, formula, array=False):\n",
    "        self.formula = formula\n",
    "        self.array = array\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        df = Formula(self.formula).get_model_matrix(X)\n",
    "        return df.values if self.array else df\n",
    "\n",
    "transformer = FormulaTransformer(\n",
    "    \"0 + poly(age, degree=6, raw=True) + poly(inc, degree=8, raw=True) \"\n",
    "    \"+ poly(educ, degree=4, raw=True) + poly(fsize, degree=2, raw=True) \"\n",
    "    \"+ male + marr + twoearn + db + pira + hown\", array=True)\n",
    "\n",
    "# IRM implementation\n",
    "def irm(X, D, y, model_y0, model_y1, model_d, *, trimming=0.01, nfolds=5):\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhat0, yhat1 = np.zeros(y.shape), np.zeros(y.shape)\n",
    "    for train, test in cv.split(X, y):\n",
    "        mdl0 = clone(model_y0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "        yhat0[test] = mdl0.predict(X.iloc[test])\n",
    "        mdl1 = clone(model_y1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1])\n",
    "        yhat1[test] = mdl1.predict(X.iloc[test])\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "    Dhat = cross_val_predict(model_d, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var = np.var(drhat)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr\n",
    "\n",
    "# Running multiple models for both income groups\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "models = {\n",
    "    \"lasso/logistic\": (make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv)),\n",
    "                         make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv)),\n",
    "                         make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))),\n",
    "    \"random forest\": (make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10)),\n",
    "                       make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10)),\n",
    "                       make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10))),\n",
    "    \"decision tree\": (make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10)),\n",
    "                       make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10)),\n",
    "                       make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10))),\n",
    "    \"boosted forest\": (make_pipeline(transformer, GradientBoostingRegressor(max_depth=2)),\n",
    "                        make_pipeline(transformer, GradientBoostingRegressor(max_depth=2)),\n",
    "                        make_pipeline(transformer, GradientBoostingClassifier(max_depth=2)))\n",
    "}\n",
    "\n",
    "results_irm = {}\n",
    "for group, (X_group, D_group, y_group) in {\"bottom25%\": (X_bottom, D_bottom, y_bottom), \"top25%\": (X_top, D_top, y_top)}.items():\n",
    "    for name, (model_y0, model_y1, model_d) in models.items():\n",
    "        point, stderr = irm(X_group, D_group, y_group, model_y0, model_y1, model_d, nfolds=5)\n",
    "        results_irm[f\"{group} {name}\"] = (point, stderr)\n",
    "\n",
    "# Print IRM results\n",
    "for name, (point, stderr) in results_irm.items():\n",
    "    print(f\"{name}: IRM Estimate = {point:.3f}, StdErr = {stderr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heterogeneity in 401(k) Effects by Income Group\n",
    "Our results indicate significant heterogeneity in the effect of 401(k) eligibility on net total financial assets (net_tfa) across income groups:\n",
    "- **Higher estimated treatment effects** for the top 25% income group compared to the bottom 25%.\n",
    "- **Larger standard errors** in the higher income group, indicating greater variability in estimates.\n",
    "\n",
    "#### Consistency Across ML Methods\n",
    "- Most ML models provide relatively consistent estimates, with some variation in decision trees due to high variance.\n",
    "- The **random forest and boosted forest models** appear to provide the most stable estimates.\n",
    "- **Decision trees show extreme variance**, particularly in the top 25% group, suggesting potential instability in simpler tree-based models.\n",
    "\n",
    "#### Implications\n",
    "- **401(k) participation has a stronger impact on higher-income individuals**, likely due to their ability to contribute more.\n",
    "- **Lower-income groups still benefit**, but the effect size is smaller, possibly due to liquidity constraints.\n",
    "- **ML methods provide robust results**, and using an ensemble approach like stacked models could further improve stability.\n",
    "\n",
    "Overall, these results highlight the importance of considering income heterogeneity when evaluating the impact of retirement policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLR (DML) Results:\n",
      "                                             estimate       stderr  \\\n",
      "select-best (semi-cfit) PLR               8753.081278  1314.454399   \n",
      "select-best (semi-cfit) PLR (bottom 25%)  -341.394317  1220.758128   \n",
      "select-best (semi-cfit) PLR (top 25%)     8033.711460  4301.654930   \n",
      "\n",
      "                                                lower         upper  \\\n",
      "select-best (semi-cfit) PLR               6176.750656  11329.411901   \n",
      "select-best (semi-cfit) PLR (bottom 25%) -2734.080248   2051.291614   \n",
      "select-best (semi-cfit) PLR (top 25%)     -397.532203  16464.955124   \n",
      "\n",
      "                                                rmse y    rmse D  accuracy D  \n",
      "select-best (semi-cfit) PLR               54334.291904  0.443427    0.690772  \n",
      "select-best (semi-cfit) PLR (bottom 25%)  20697.433220  0.425186    0.734394  \n",
      "select-best (semi-cfit) PLR (top 25%)     96455.132005  0.463909    0.666398  \n",
      "\n",
      "IRM Results:\n",
      "                                                        estimate       stderr\n",
      "select best IRM with semi cross fitting all sam...   7497.464329  1197.459768\n",
      "select best IRM with semi cross fitting bottom ...  -1017.811692  1422.038571\n",
      "select best IRM with semi cross fitting top 25%...  12221.606026  5876.072632\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LassoCV, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor,\n",
    "                              RandomForestClassifier, GradientBoostingClassifier)\n",
    "from formulaic import Formula\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, clone\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from copy import deepcopy\n",
    "\n",
    "# ---------------------------\n",
    "# Set seed for reproducibility\n",
    "# ---------------------------\n",
    "np.random.seed(1234)\n",
    "\n",
    "# ---------------------------\n",
    "# Define a summary function that does not depend on any external synth object.\n",
    "# It simply computes confidence intervals, RMSE for outcome and treatment predictions,\n",
    "# and (if D is binary) classification accuracy.\n",
    "# ---------------------------\n",
    "def summary(point, stderr, yhat, Dhat, resy, resD, final_residual, X, D, y, *, name, synth=None):\n",
    "    lower = point - 1.96 * stderr\n",
    "    upper = point + 1.96 * stderr\n",
    "    rmse_y = np.sqrt(mean_squared_error(y, yhat))\n",
    "    rmse_D = np.sqrt(mean_squared_error(D, Dhat))\n",
    "    # If treatment is binary and Dhat are predicted probabilities:\n",
    "    predicted_D = (Dhat >= 0.5).astype(int)\n",
    "    accuracy_D = np.mean(predicted_D == D)\n",
    "    \n",
    "    data = {\n",
    "        \"estimate\": point,\n",
    "        \"stderr\": stderr,\n",
    "        \"lower\": lower,\n",
    "        \"upper\": upper,\n",
    "        \"rmse y\": rmse_y,\n",
    "        \"rmse D\": rmse_D,\n",
    "        \"accuracy D\": accuracy_D\n",
    "    }\n",
    "    return pd.DataFrame(data, index=[name])\n",
    "\n",
    "# ---------------------------\n",
    "# Define a simple dml() function.\n",
    "# This version uses cross‑fitting to generate OOF predictions for Y and D,\n",
    "# then computes the DML estimator as theta = E[(y – yhat)*(D – Dhat)] / E[(D – Dhat)^2]\n",
    "# and a standard error based on a simplified variance estimator.\n",
    "# ---------------------------\n",
    "def dml(X, D, y, y_model, d_model, classifier=False, nfolds=5):\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=1234)\n",
    "    # Outcome predictions\n",
    "    if classifier:\n",
    "        yhat = cross_val_predict(y_model, X, y, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    else:\n",
    "        yhat = cross_val_predict(y_model, X, y, cv=cv, n_jobs=-1)\n",
    "    # Treatment predictions (always using predict_proba since D is binary)\n",
    "    Dhat = cross_val_predict(d_model, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    \n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "    theta = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    final_residual = resy - theta * resD\n",
    "    var = np.mean(final_residual**2 * resD**2) / (np.mean(resD**2)**2)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return theta, stderr, yhat, Dhat, resy, resD, final_residual\n",
    "\n",
    "# ---------------------------\n",
    "# Define a simple irm() function.\n",
    "# Here we compute a “doubly robust” (IRM) score in a simplified manner.\n",
    "# (This is only one possible implementation; adjust as needed.)\n",
    "# ---------------------------\n",
    "def irm(X, D, y, y_model, d_model, nfolds=5):\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=1234)\n",
    "    # Outcome prediction (here using the same model for all observations)\n",
    "    yhat = cross_val_predict(y_model, X, y, cv=cv, n_jobs=-1)\n",
    "    # Propensity score predictions:\n",
    "    Dhat = cross_val_predict(d_model, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    # Compute a doubly robust score (note: this is a simplified version)\n",
    "    dr_score = (y - yhat) * (D - Dhat) / (Dhat * (1 - Dhat) + 1e-6)\n",
    "    theta = np.mean(dr_score)\n",
    "    stderr = np.std(dr_score) / np.sqrt(len(dr_score))\n",
    "    return theta, stderr\n",
    "\n",
    "# ---------------------------\n",
    "# For model selection we define a helper that computes OOF predictions and MSE.\n",
    "# ---------------------------\n",
    "def get_oof_predictions(model, X, y, cv, classifier=False):\n",
    "    if classifier:\n",
    "        preds = cross_val_predict(model, X, y, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    else:\n",
    "        preds = cross_val_predict(model, X, y, cv=cv, n_jobs=-1)\n",
    "    mse = mean_squared_error(y, preds)\n",
    "    return preds, mse\n",
    "\n",
    "def dml_select_best(X, D, y, model_y_list, model_d_list, nfolds=5, classifier=False):\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=1234)\n",
    "    best_mse_y = np.inf\n",
    "    best_y_model = None\n",
    "    best_yhat = None\n",
    "    for candidate in model_y_list:\n",
    "        candidate_clone = deepcopy(candidate)\n",
    "        preds, mse_val = get_oof_predictions(candidate_clone, X, y, cv, classifier=False)\n",
    "        if mse_val < best_mse_y:\n",
    "            best_mse_y = mse_val\n",
    "            best_yhat = preds\n",
    "            best_y_model = candidate_clone\n",
    "    best_mse_d = np.inf\n",
    "    best_d_model = None\n",
    "    best_Dhat = None\n",
    "    for candidate in model_d_list:\n",
    "        candidate_clone = deepcopy(candidate)\n",
    "        preds, mse_val = get_oof_predictions(candidate_clone, X, D, cv, classifier=True)\n",
    "        if mse_val < best_mse_d:\n",
    "            best_mse_d = mse_val\n",
    "            best_Dhat = preds\n",
    "            best_d_model = candidate_clone\n",
    "    resy = y - best_yhat\n",
    "    resD = D - best_Dhat\n",
    "    theta = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    final_residual = resy - theta * resD\n",
    "    var = np.mean(final_residual**2 * resD**2) / (np.mean(resD**2)**2)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return theta, stderr, best_yhat, best_Dhat, resy, resD, final_residual\n",
    "\n",
    "# ---------------------------\n",
    "# Define candidate models and a Formula-based transformer.\n",
    "# ---------------------------\n",
    "model_y_list = [\n",
    "    make_pipeline(StandardScaler(), LassoCV(cv=5)),\n",
    "    make_pipeline(StandardScaler(), RandomForestRegressor(n_estimators=100, min_samples_leaf=10)),\n",
    "    make_pipeline(StandardScaler(), GradientBoostingRegressor(max_depth=2))\n",
    "]\n",
    "model_d_list = [\n",
    "    make_pipeline(StandardScaler(), LogisticRegressionCV(cv=5)),\n",
    "    make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100, min_samples_leaf=10)),\n",
    "    make_pipeline(StandardScaler(), GradientBoostingClassifier(max_depth=2))\n",
    "]\n",
    "\n",
    "class FormulaTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, formula, array=False):\n",
    "        self.formula = formula\n",
    "        self.array = array\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        df = Formula(self.formula).get_model_matrix(X)\n",
    "        return df.values if self.array else df\n",
    "\n",
    "formula_str = (\n",
    "    \"0 + poly(age, degree=6, raw=True) + poly(inc, degree=8, raw=True) \"\n",
    "    \"+ poly(educ, degree=4, raw=True) + poly(fsize, degree=2, raw=True) \"\n",
    "    \"+ male + marr + twoearn + db + pira + hown\"\n",
    ")\n",
    "transformer = FormulaTransformer(formula=formula_str, array=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Load data and create income subgroups.\n",
    "# ---------------------------\n",
    "url = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv\"\n",
    "df = pd.read_csv(url)\n",
    "treatment = 'e401'\n",
    "outcome = 'net_tfa'\n",
    "\n",
    "low_income_threshold = df['net_tfa'].quantile(0.25)\n",
    "high_income_threshold = df['net_tfa'].quantile(0.75)\n",
    "df_low_income = df[df['net_tfa'] <= low_income_threshold]\n",
    "df_high_income = df[df['net_tfa'] >= high_income_threshold]\n",
    "\n",
    "# Transform features for each group\n",
    "X_full = transformer.transform(df)\n",
    "X_low = transformer.transform(df_low_income)\n",
    "X_high = transformer.transform(df_high_income)\n",
    "D_full = df[treatment]\n",
    "y_full = df[outcome]\n",
    "D_low = df_low_income[treatment]\n",
    "y_low = df_low_income[outcome]\n",
    "D_high = df_high_income[treatment]\n",
    "y_high = df_high_income[outcome]\n",
    "\n",
    "# ---------------------------\n",
    "# PLR (DML) Analysis: Full sample, Bottom 25%, and Top 25%\n",
    "# ---------------------------\n",
    "theta_full, stderr_full, yhat_full, Dhat_full, resy_full, resD_full, err_full = dml_select_best(\n",
    "    X_full, D_full, y_full, model_y_list, model_d_list, nfolds=5, classifier=True\n",
    ")\n",
    "table_plr_full = summary(theta_full, stderr_full, yhat_full, Dhat_full, resy_full, resD_full, err_full,\n",
    "                         X_full, D_full, y_full, name='select-best (semi-cfit) PLR', synth=None)\n",
    "\n",
    "theta_low, stderr_low, yhat_low, Dhat_low, resy_low, resD_low, err_low = dml_select_best(\n",
    "    X_low, D_low, y_low, model_y_list, model_d_list, nfolds=5, classifier=True\n",
    ")\n",
    "table_plr_low = summary(theta_low, stderr_low, yhat_low, Dhat_low, resy_low, resD_low, err_low,\n",
    "                        X_low, D_low, y_low, name='select-best (semi-cfit) PLR (bottom 25%)', synth=None)\n",
    "\n",
    "theta_high, stderr_high, yhat_high, Dhat_high, resy_high, resD_high, err_high = dml_select_best(\n",
    "    X_high, D_high, y_high, model_y_list, model_d_list, nfolds=5, classifier=True\n",
    ")\n",
    "table_plr_high = summary(theta_high, stderr_high, yhat_high, Dhat_high, resy_high, resD_high, err_high,\n",
    "                         X_high, D_high, y_high, name='select-best (semi-cfit) PLR (top 25%)', synth=None)\n",
    "\n",
    "table_plr = pd.concat([table_plr_full, table_plr_low, table_plr_high])\n",
    "print(\"PLR (DML) Results:\")\n",
    "print(table_plr)\n",
    "\n",
    "# ---------------------------\n",
    "# IRM Analysis: Full sample, Bottom 25%, and Top 25%\n",
    "# ---------------------------\n",
    "theta_irm_full, stderr_irm_full = irm(X_full, D_full, y_full, model_y_list[0], model_d_list[0], nfolds=5)\n",
    "table_irm_full = pd.DataFrame({\"estimate\": [theta_irm_full], \"stderr\": [stderr_irm_full]},\n",
    "                              index=[\"select best IRM with semi cross fitting all samples\"])\n",
    "\n",
    "theta_irm_low, stderr_irm_low = irm(X_low, D_low, y_low, model_y_list[0], model_d_list[0], nfolds=5)\n",
    "table_irm_low = pd.DataFrame({\"estimate\": [theta_irm_low], \"stderr\": [stderr_irm_low]},\n",
    "                             index=[\"select best IRM with semi cross fitting bottom 25% income\"])\n",
    "\n",
    "theta_irm_high, stderr_irm_high = irm(X_high, D_high, y_high, model_y_list[0], model_d_list[0], nfolds=5)\n",
    "table_irm_high = pd.DataFrame({\"estimate\": [theta_irm_high], \"stderr\": [stderr_irm_high]},\n",
    "                              index=[\"select best IRM with semi cross fitting top 25% income\"])\n",
    "\n",
    "table_irm = pd.concat([table_irm_full, table_irm_low, table_irm_high])\n",
    "print(\"\\nIRM Results:\")\n",
    "print(table_irm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- EconML LinearDML (PLR) -----------------\n",
      "Random Forest in EconML for PLR:\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8597.357 1333.637 6.447    0.0 5983.477 11211.237\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "\n",
      "Gradient Boosting in EconML for PLR:\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr zstat pvalue ci_lower  ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       9129.167 1379.02  6.62    0.0 6426.337 11831.997\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "\n",
      "Decision Tree in EconML for PLR:\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8772.347 1448.641 6.056    0.0 5933.064 11611.631\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "\n",
      "----------------- DoubleML PLR -----------------\n",
      "Random Forest in DoubleML for PLR:\n",
      "         coef      std err        t         P>|t|        2.5 %        97.5 %\n",
      "d  8899.06343  1350.321324  6.59033  4.388494e-11  6252.482268  11545.644592\n",
      "\n",
      "Decision Tree in DoubleML for PLR:\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  9437.337876  1442.845685  6.540781  6.119833e-11  6609.412297  12265.263454\n",
      "\n",
      "Gradient Boosting in DoubleML for PLR:\n",
      "          coef     std err         t         P>|t|        2.5 %       97.5 %\n",
      "d  8713.070355  1329.35067  6.554381  5.587327e-11  6107.590919  11318.54979\n",
      "\n",
      "----------------- EconML LinearDRLearner (IRM) -----------------\n",
      "Random Forest in EconML for IRM:\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       7815.297 1139.885 6.856    0.0 5581.164 10049.431\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "\n",
      "Decision Tree in EconML for IRM:\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       8019.136 1210.989 6.622    0.0 5645.642 10392.63\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "\n",
      "Gradient Boosting in EconML for IRM:\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8238.929 1139.019 7.233    0.0 6006.492 10471.366\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "\n",
      "----------------- DoubleML IRM -----------------\n",
      "Random Forest in DoubleML for IRM:\n",
      "          coef      std err         t         P>|t|        2.5 %       97.5 %\n",
      "d  7489.651913  1135.094875  6.598261  4.160093e-11  5264.906839  9714.396987\n",
      "\n",
      "Decision Tree in DoubleML for IRM:\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  7868.336432  1184.937034  6.640299  3.130471e-11  5545.902521  10190.770343\n",
      "\n",
      "Gradient Boosting in DoubleML for IRM:\n",
      "          coef     std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  8534.094502  1129.18323  7.557759  4.100735e-14  6320.936039  10747.252965\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (RandomForestRegressor, RandomForestClassifier,\n",
    "                              GradientBoostingRegressor, GradientBoostingClassifier)\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from econml.dml import LinearDML\n",
    "from econml.dr import LinearDRLearner\n",
    "from doubleml import DoubleMLData, DoubleMLPLR, DoubleMLIRM\n",
    "import doubleml as dbml\n",
    "from formulaic import Formula\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "# ---------------------------\n",
    "# Data and Feature Transformation\n",
    "# ---------------------------\n",
    "# Load 401(k) dataset\n",
    "url = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv\"\n",
    "df = pd.read_csv(url)\n",
    "y = df['net_tfa'].values\n",
    "D = df['e401'].values  # treatment variable (e401)\n",
    "\n",
    "# Define a transformer using the formulaic package (your existing transformer)\n",
    "class FormulaTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, formula, array=False):\n",
    "        self.formula = formula\n",
    "        self.array = array\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        model_matrix = Formula(self.formula).get_model_matrix(X)\n",
    "        return model_matrix.values if self.array else model_matrix\n",
    "\n",
    "formula_str = (\n",
    "    \"0 + poly(age, degree=6, raw=True) + poly(inc, degree=8, raw=True) \"\n",
    "    \"+ poly(educ, degree=4, raw=True) + poly(fsize, degree=2, raw=True) \"\n",
    "    \"+ male + marr + twoearn + db + pira + hown\"\n",
    ")\n",
    "transformer = FormulaTransformer(formula=formula_str, array=True)\n",
    "\n",
    "# Create feature matrix W and standardize it\n",
    "X = transformer.fit_transform(df)\n",
    "W = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Prepare DoubleML data object (for DoubleML methods)\n",
    "dml_data = DoubleMLData.from_arrays(W, y, D)\n",
    "\n",
    "# ---------------------------\n",
    "# EconML PLR Variants (LinearDML)\n",
    "# ---------------------------\n",
    "print(\"----------------- EconML LinearDML (PLR) -----------------\")\n",
    "\n",
    "# Random Forest variant\n",
    "ldml_rf = LinearDML(\n",
    "    model_y=RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    model_t=RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"Random Forest in EconML for PLR:\")\n",
    "print(ldml_rf.summary())\n",
    "\n",
    "# Gradient Boosting variant\n",
    "ldml_gb = LinearDML(\n",
    "    model_y=GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    model_t=GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"\\nGradient Boosting in EconML for PLR:\")\n",
    "print(ldml_gb.summary())\n",
    "\n",
    "# Decision Tree variant\n",
    "ldml_dt = LinearDML(\n",
    "    model_y=DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    model_t=DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"\\nDecision Tree in EconML for PLR:\")\n",
    "print(ldml_dt.summary())\n",
    "\n",
    "# ---------------------------\n",
    "# DoubleML PLR Variants (DoubleMLPLR)\n",
    "# ---------------------------\n",
    "print(\"\\n----------------- DoubleML PLR -----------------\")\n",
    "\n",
    "# Random Forest variant\n",
    "dml_plr_rf = DoubleMLPLR(\n",
    "    dml_data,\n",
    "    RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_plr_rf.fit()\n",
    "print(\"Random Forest in DoubleML for PLR:\")\n",
    "print(dml_plr_rf.summary)\n",
    "\n",
    "# Decision Tree variant\n",
    "dml_plr_dt = DoubleMLPLR(\n",
    "    dml_data,\n",
    "    DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_plr_dt.fit()\n",
    "print(\"\\nDecision Tree in DoubleML for PLR:\")\n",
    "print(dml_plr_dt.summary)\n",
    "\n",
    "# Gradient Boosting variant\n",
    "dml_plr_gb = DoubleMLPLR(\n",
    "    dml_data,\n",
    "    GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_plr_gb.fit()\n",
    "print(\"\\nGradient Boosting in DoubleML for PLR:\")\n",
    "print(dml_plr_gb.summary)\n",
    "\n",
    "# ---------------------------\n",
    "# EconML IRM Variants (LinearDRLearner)\n",
    "# ---------------------------\n",
    "print(\"\\n----------------- EconML LinearDRLearner (IRM) -----------------\")\n",
    "\n",
    "# Random Forest variant\n",
    "dr_rf = LinearDRLearner(\n",
    "    model_regression=RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    model_propensity=RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    cv=5,\n",
    ").fit(y, D, W=W)\n",
    "print(\"Random Forest in EconML for IRM:\")\n",
    "print(dr_rf.summary(T=1))\n",
    "\n",
    "# Decision Tree variant\n",
    "dr_dt = LinearDRLearner(\n",
    "    model_regression=DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    model_propensity=DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    cv=5,\n",
    ").fit(y, D, W=W)\n",
    "print(\"\\nDecision Tree in EconML for IRM:\")\n",
    "print(dr_dt.summary(T=1))\n",
    "\n",
    "# Gradient Boosting variant\n",
    "dr_gb = LinearDRLearner(\n",
    "    model_regression=GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    model_propensity=GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    cv=5,\n",
    ").fit(y, D, W=W)\n",
    "print(\"\\nGradient Boosting in EconML for IRM:\")\n",
    "print(dr_gb.summary(T=1))\n",
    "\n",
    "# ---------------------------\n",
    "# DoubleML IRM Variants (DoubleMLIRM)\n",
    "# ---------------------------\n",
    "print(\"\\n----------------- DoubleML IRM -----------------\")\n",
    "\n",
    "# Random Forest variant\n",
    "dml_irm_rf = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_rf.fit()\n",
    "print(\"Random Forest in DoubleML for IRM:\")\n",
    "print(dml_irm_rf.summary)\n",
    "\n",
    "# Decision Tree variant\n",
    "dml_irm_dt = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=0.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_dt.fit()\n",
    "print(\"\\nDecision Tree in DoubleML for IRM:\")\n",
    "print(dml_irm_dt.summary)\n",
    "\n",
    "# Gradient Boosting variant\n",
    "dml_irm_gb = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_gb.fit()\n",
    "print(\"\\nGradient Boosting in DoubleML for IRM:\")\n",
    "print(dml_irm_gb.summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that, for the individual base‐learner variants—using Random Forest, Gradient Boosting, and Decision Tree as the outcome and treatment models—both EconML and DoubleML yield estimates (and standard errors) that are broadly consistent with the custom semi‑crossfitting code. In other words, when you plug in a single learner for the outcome model and a single learner for the treatment model, both libraries produce comparable point estimates and inference for the PLR (LinearDML/DoubleMLPLR) and IRM (LinearDRLearner/DoubleMLIRM) approaches.\n",
    "\n",
    "What’s replicable:\n",
    "\n",
    "Single-Model Variants:\n",
    "- Random Forest, Decision Tree, and Gradient Boosting variants for both PLR and IRM.\n",
    "- The estimates and standard errors from EconML (e.g. intercept results from LinearDML and LinearDRLearner) are similar to those from DoubleML’s PLR and IRM implementations.\n",
    "\n",
    "Standard Plug-In Approach:\n",
    "- When I specify a single model for each component (outcome and treatment), both EconML and DoubleML handle cross‑fitting internally and provide consistent inference, as seen in the outputs.\n",
    "\n",
    "What’s not directly replicable:\n",
    "\n",
    "- Custom Stacking/Semi‑Crossfitting:\n",
    "In my custom code I implemented a stacking/ model selection procedure that chooses the best model among a list of candidate models via semi‑crossfitting. Neither EconML nor DoubleML currently offers a built‑in option for that kind of model selection.\n",
    "These packages expect you to supply a single learner for the outcome and treatment equations rather than automatically selecting among multiple candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Semi-Synthetic Data with n=1000 ===\n",
      "True ATE in the semi-synthetic world: 18700.470823845746\n",
      "\n",
      "** PLR Results **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.164e+11, tolerance: 3.852e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.957e+11, tolerance: 3.692e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.351e+11, tolerance: 4.268e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.647e+11, tolerance: 3.361e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.857e+11, tolerance: 2.884e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.164e+11, tolerance: 3.852e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.957e+11, tolerance: 3.692e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.351e+11, tolerance: 4.268e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.647e+11, tolerance: 3.361e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.857e+11, tolerance: 2.884e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.164e+11, tolerance: 3.852e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.957e+11, tolerance: 3.692e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.351e+11, tolerance: 4.268e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.647e+11, tolerance: 3.361e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.857e+11, tolerance: 2.884e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.164e+11, tolerance: 3.852e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.957e+11, tolerance: 3.692e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.351e+11, tolerance: 4.268e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.647e+11, tolerance: 3.361e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.857e+11, tolerance: 2.884e+09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             estimate       stderr         lower  \\\n",
      "double lasso             13002.839853  3377.349913   6383.234023   \n",
      "lasso/logistic           14987.982400  3487.900394   8151.697628   \n",
      "random forest            18326.225913  3867.595277  10745.739169   \n",
      "decision tree            15727.327769  3496.528392   8874.132120   \n",
      "boosted forest           16267.271162  3666.128307   9081.659681   \n",
      "automl (semi-cfit)       18326.225913  3867.595277  10745.739169   \n",
      "stacked (semi-cfit)      15914.627706  3390.832616   9268.595777   \n",
      "select-best (semi-cfit)  14987.982400  3487.900394   8151.697628   \n",
      "\n",
      "                                upper        rmse y    rmse D  accuracy D  \\\n",
      "double lasso             19622.445683  38975.918146  0.362264       0.856   \n",
      "lasso/logistic           21824.267171  38975.918146  0.350154       0.848   \n",
      "random forest            25906.712657  43456.027682  0.351388       0.856   \n",
      "decision tree            22580.523418  39079.097211  0.349911       0.856   \n",
      "boosted forest           23452.882643  41057.871157  0.350716       0.856   \n",
      "automl (semi-cfit)       25906.712657  43456.027682  0.351388       0.856   \n",
      "stacked (semi-cfit)      22560.659634  37660.747956  0.347417       0.855   \n",
      "select-best (semi-cfit)  21824.267171  38975.918146  0.350154       0.848   \n",
      "\n",
      "                               error  rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "double lasso             5697.630970   23589.830264     0.134381        1  \n",
      "lasso/logistic           3712.488424   23589.830264     0.075688        1  \n",
      "random forest             374.244911   30458.719356     0.104785        1  \n",
      "decision tree            2973.143055   23742.179712     0.089943        1  \n",
      "boosted forest           2433.199661   26895.530738     0.098872        1  \n",
      "automl (semi-cfit)        573.138423   39469.639658     0.100936        1  \n",
      "stacked (semi-cfit)      1838.459784   31270.777956     0.077401        1  \n",
      "select-best (semi-cfit)  2765.105090   34702.450198     0.076591        1  \n",
      "\n",
      "** IRM Results **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.136e+11, tolerance: 1.605e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.054e+10, tolerance: 1.976e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.390e+11, tolerance: 1.745e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+10, tolerance: 1.702e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.325e+11, tolerance: 1.729e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.689e+10, tolerance: 2.252e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.052e+11, tolerance: 1.378e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.871e+10, tolerance: 1.707e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+10, tolerance: 7.095e+08\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+10, tolerance: 1.896e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.136e+11, tolerance: 1.605e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.054e+10, tolerance: 1.976e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.390e+11, tolerance: 1.745e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+10, tolerance: 1.702e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.325e+11, tolerance: 1.729e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.689e+10, tolerance: 2.252e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.052e+11, tolerance: 1.378e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.871e+10, tolerance: 1.707e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+10, tolerance: 7.095e+08\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+10, tolerance: 1.896e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.136e+11, tolerance: 1.605e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.054e+10, tolerance: 1.976e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.390e+11, tolerance: 1.745e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+10, tolerance: 1.702e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.325e+11, tolerance: 1.729e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.689e+10, tolerance: 2.252e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.052e+11, tolerance: 1.378e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.871e+10, tolerance: 1.707e+09\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+10, tolerance: 7.095e+08\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+10, tolerance: 1.896e+09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             estimate       stderr         lower  \\\n",
      "lasso/logistic            9933.012692  9568.041690  -8820.349021   \n",
      "random forest            19799.627854  2850.773209  14212.112364   \n",
      "decision tree            18382.758945  2588.288387  13309.713707   \n",
      "boosted forest           18792.825869  3190.334905  12539.769455   \n",
      "automl (semi-cfit)       19799.627854  2850.773209  14212.112364   \n",
      "stacked (semi-cfit)      17761.471575  2737.088108  12396.778883   \n",
      "select-best (semi-cfit)   9933.012692  9568.041690  -8820.349021   \n",
      "\n",
      "                                upper        rmse y    rmse D  accuracy D  \\\n",
      "lasso/logistic           28686.374405  36886.134760  0.350154       0.848   \n",
      "random forest            25387.143344  46420.643405  0.351388       0.856   \n",
      "decision tree            23455.804184  41980.132269  0.349911       0.856   \n",
      "boosted forest           25045.882283  44304.197790  0.350716       0.856   \n",
      "automl (semi-cfit)       25387.143344  46420.643405  0.351388       0.856   \n",
      "stacked (semi-cfit)      23126.164267  39439.286583  0.347417       0.855   \n",
      "select-best (semi-cfit)  28686.374405  36886.134760  0.350154       0.848   \n",
      "\n",
      "                               error  rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "lasso/logistic           7820.074798   32411.918441     0.076591        1  \n",
      "random forest            2046.540364   45890.514627     0.100936        1  \n",
      "decision tree             629.671455   37624.039485     0.086864        1  \n",
      "boosted forest           1039.738379   42067.999890     0.095344        1  \n",
      "automl (semi-cfit)       2046.540364   45890.514627     0.100936        1  \n",
      "stacked (semi-cfit)         8.384085   36479.204822     0.077401        1  \n",
      "select-best (semi-cfit)  7820.074798   32411.918441     0.076591        1  \n",
      "\n",
      "=== Semi-Synthetic Data with n=10000 ===\n",
      "True ATE in the semi-synthetic world: 18700.470823845746\n",
      "\n",
      "** PLR Results **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.430e+11, tolerance: 3.409e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.670e+12, tolerance: 3.212e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.117e+12, tolerance: 3.222e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.290e+11, tolerance: 3.523e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.336e+12, tolerance: 3.322e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.430e+11, tolerance: 3.409e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.670e+12, tolerance: 3.212e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.117e+12, tolerance: 3.222e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.290e+11, tolerance: 3.523e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.336e+12, tolerance: 3.322e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.430e+11, tolerance: 3.409e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.670e+12, tolerance: 3.212e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.117e+12, tolerance: 3.222e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.290e+11, tolerance: 3.523e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.336e+12, tolerance: 3.322e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.430e+11, tolerance: 3.409e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.670e+12, tolerance: 3.212e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.117e+12, tolerance: 3.222e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.290e+11, tolerance: 3.523e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.336e+12, tolerance: 3.322e+10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             estimate       stderr         lower  \\\n",
      "double lasso             12248.361660  1061.615107  10167.596050   \n",
      "lasso/logistic           14203.404629  1129.831245  11988.935389   \n",
      "random forest            14176.748486   929.822738  12354.295920   \n",
      "decision tree            14659.814393   917.233710  12862.036320   \n",
      "boosted forest           14427.055021   919.988433  12623.877692   \n",
      "automl (semi-cfit)       14176.748486   929.822738  12354.295920   \n",
      "stacked (semi-cfit)      14453.908008   932.722041  12625.772807   \n",
      "select-best (semi-cfit)  14203.404629  1129.831245  11988.935389   \n",
      "\n",
      "                                upper        rmse y    rmse D  accuracy D  \\\n",
      "double lasso             14329.127270  37535.127746  0.351236      0.8659   \n",
      "lasso/logistic           16417.873869  37535.127746  0.329624      0.8658   \n",
      "random forest            15999.201053  31075.036393  0.330386      0.8659   \n",
      "decision tree            16457.592465  30927.286539  0.332954      0.8661   \n",
      "boosted forest           16230.232349  30889.280220  0.331703      0.8663   \n",
      "automl (semi-cfit)       15999.201053  31075.036393  0.330386      0.8659   \n",
      "stacked (semi-cfit)      16282.043209  31138.141810  0.329904      0.8659   \n",
      "select-best (semi-cfit)  16417.873869  37535.127746  0.329624      0.8658   \n",
      "\n",
      "                               error  rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "double lasso             6452.109164   24060.983324     0.124210        0  \n",
      "lasso/logistic           4497.066195   24060.983324     0.031981        0  \n",
      "random forest            4523.722337   11126.284616     0.037925        0  \n",
      "decision tree            4040.656431   10737.075696     0.055731        0  \n",
      "boosted forest           4273.415803   10619.716725     0.047707        0  \n",
      "automl (semi-cfit)       4642.757822   11324.357564     0.038472        0  \n",
      "stacked (semi-cfit)      4365.598300   12348.222792     0.035638        0  \n",
      "select-best (semi-cfit)  4616.101679   25310.413302     0.037564        0  \n",
      "\n",
      "** IRM Results **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.301e+12, tolerance: 1.278e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.355e+11, tolerance: 1.866e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.466e+11, tolerance: 1.137e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.365e+11, tolerance: 1.803e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.229e+12, tolerance: 1.359e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.017e+11, tolerance: 1.612e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.268e+12, tolerance: 1.400e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.842e+11, tolerance: 1.866e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.332e+12, tolerance: 1.370e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.815e+11, tolerance: 1.702e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.301e+12, tolerance: 1.278e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.355e+11, tolerance: 1.866e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.466e+11, tolerance: 1.137e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.365e+11, tolerance: 1.803e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.229e+12, tolerance: 1.359e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.017e+11, tolerance: 1.612e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.268e+12, tolerance: 1.400e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.842e+11, tolerance: 1.866e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.332e+12, tolerance: 1.370e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.815e+11, tolerance: 1.702e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.301e+12, tolerance: 1.278e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.355e+11, tolerance: 1.866e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.466e+11, tolerance: 1.137e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.365e+11, tolerance: 1.803e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.229e+12, tolerance: 1.359e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.017e+11, tolerance: 1.612e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.268e+12, tolerance: 1.400e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.842e+11, tolerance: 1.866e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.332e+12, tolerance: 1.370e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.815e+11, tolerance: 1.702e+10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             estimate       stderr         lower  \\\n",
      "lasso/logistic           14896.535428  2124.723388  10732.077586   \n",
      "random forest            15504.740650  1247.892910  13058.870547   \n",
      "decision tree            14772.919735  2485.747394   9900.854843   \n",
      "boosted forest           15298.029096  1737.701851  11892.133469   \n",
      "automl (semi-cfit)       15504.740650  1247.892910  13058.870547   \n",
      "stacked (semi-cfit)      15229.611550  1508.937667  12272.093724   \n",
      "select-best (semi-cfit)  14896.535428  2124.723388  10732.077586   \n",
      "\n",
      "                                upper        rmse y    rmse D  accuracy D  \\\n",
      "lasso/logistic           19060.993269  36343.850947  0.329624      0.8658   \n",
      "random forest            17950.610754  31651.101376  0.330386      0.8659   \n",
      "decision tree            19644.984627  29725.229718  0.332954      0.8661   \n",
      "boosted forest           18703.924723  30563.990608  0.331703      0.8663   \n",
      "automl (semi-cfit)       17950.610754  31651.101376  0.330386      0.8659   \n",
      "stacked (semi-cfit)      18187.129377  30391.168826  0.329904      0.8659   \n",
      "select-best (semi-cfit)  19060.993269  36343.850947  0.329624      0.8658   \n",
      "\n",
      "                               error  rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "lasso/logistic           3922.970880   23206.939378     0.037564        1  \n",
      "random forest            3314.765658   12697.649783     0.038472        0  \n",
      "decision tree            4046.586573    7197.424877     0.056542        1  \n",
      "boosted forest           3521.477212    9688.983879     0.048358        0  \n",
      "automl (semi-cfit)       3314.765658   12697.649783     0.038472        0  \n",
      "stacked (semi-cfit)      3589.894758    9994.369984     0.035638        0  \n",
      "select-best (semi-cfit)  3922.970880   23206.939378     0.037564        1  \n",
      "\n",
      "=== Semi-Synthetic Data with n=50000 ===\n",
      "True ATE in the semi-synthetic world: 18700.470823845746\n",
      "\n",
      "** PLR Results **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.869e+12, tolerance: 1.836e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.077e+12, tolerance: 1.784e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.564e+12, tolerance: 1.802e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.172e+12, tolerance: 1.786e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.495e+12, tolerance: 1.773e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.869e+12, tolerance: 1.836e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.077e+12, tolerance: 1.784e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.564e+12, tolerance: 1.802e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.172e+12, tolerance: 1.786e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.495e+12, tolerance: 1.773e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.869e+12, tolerance: 1.836e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.077e+12, tolerance: 1.784e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.564e+12, tolerance: 1.802e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.172e+12, tolerance: 1.786e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.495e+12, tolerance: 1.773e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.869e+12, tolerance: 1.836e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.077e+12, tolerance: 1.784e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.564e+12, tolerance: 1.802e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.172e+12, tolerance: 1.786e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.495e+12, tolerance: 1.773e+11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             estimate      stderr         lower         upper  \\\n",
      "double lasso             14087.034377  490.042332  13126.551406  15047.517348   \n",
      "lasso/logistic           16092.590952  522.073468  15069.326955  17115.854950   \n",
      "random forest            15643.669083  415.715392  14828.866915  16458.471250   \n",
      "decision tree            15933.068772  411.325913  15126.869982  16739.267562   \n",
      "boosted forest           15847.226717  412.681005  15038.371947  16656.081487   \n",
      "automl (semi-cfit)       15643.669083  415.715392  14828.866915  16458.471250   \n",
      "stacked (semi-cfit)      15921.851765  421.804447  15095.115050  16748.588481   \n",
      "select-best (semi-cfit)  16092.590952  522.073468  15069.326955  17115.854950   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D        error  \\\n",
      "double lasso             38915.462333  0.352244     0.86526  4613.436447   \n",
      "lasso/logistic           38915.462333  0.330231     0.86526  2607.879871   \n",
      "random forest            31220.537564  0.331203     0.86526  3056.801741   \n",
      "decision tree            31431.307976  0.336721     0.86348  2767.402052   \n",
      "boosted forest           31272.076835  0.333999     0.86448  2853.244107   \n",
      "automl (semi-cfit)       31220.537564  0.331203     0.86526  3163.268370   \n",
      "stacked (semi-cfit)      31681.891690  0.331218     0.86526  2885.085688   \n",
      "select-best (semi-cfit)  38915.462333  0.330231     0.86526  2714.346500   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "double lasso              24742.744467     0.125218        0  \n",
      "lasso/logistic            24742.744467     0.023987        0  \n",
      "random forest              8930.218776     0.034388        0  \n",
      "decision tree              9650.742414     0.069729        0  \n",
      "boosted forest             9132.561994     0.054644        0  \n",
      "automl (semi-cfit)         9515.873274     0.267557        0  \n",
      "stacked (semi-cfit)       10046.825979     0.274079        0  \n",
      "select-best (semi-cfit)   23530.387072     0.275987        0  \n",
      "\n",
      "** IRM Results **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.076e+12, tolerance: 6.563e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.533e+12, tolerance: 1.044e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.197e+12, tolerance: 6.251e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.343e+12, tolerance: 1.022e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.027e+12, tolerance: 6.214e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.450e+12, tolerance: 1.042e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.131e+12, tolerance: 6.284e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.669e+12, tolerance: 1.022e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.429e+11, tolerance: 6.068e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.738e+12, tolerance: 1.027e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.076e+12, tolerance: 6.563e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.533e+12, tolerance: 1.044e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.197e+12, tolerance: 6.251e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.343e+12, tolerance: 1.022e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.027e+12, tolerance: 6.214e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.450e+12, tolerance: 1.042e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.131e+12, tolerance: 6.284e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.669e+12, tolerance: 1.022e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.429e+11, tolerance: 6.068e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.738e+12, tolerance: 1.027e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.076e+12, tolerance: 6.563e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.533e+12, tolerance: 1.044e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.197e+12, tolerance: 6.251e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.343e+12, tolerance: 1.022e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.027e+12, tolerance: 6.214e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.450e+12, tolerance: 1.042e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.131e+12, tolerance: 6.284e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.669e+12, tolerance: 1.022e+11\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.429e+11, tolerance: 6.068e+10\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.738e+12, tolerance: 1.027e+11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             estimate      stderr         lower         upper  \\\n",
      "lasso/logistic           18327.067317  649.276486  17054.485405  19599.649229   \n",
      "random forest            18092.224689  387.041665  17333.623025  18850.826352   \n",
      "decision tree            17827.506380  608.224300  16635.386752  19019.626008   \n",
      "boosted forest           17921.165033  512.587921  16916.492707  18925.837359   \n",
      "automl (semi-cfit)       18092.224689  387.041665  17333.623025  18850.826352   \n",
      "stacked (semi-cfit)      18053.363401  454.551836  17162.441803  18944.284999   \n",
      "select-best (semi-cfit)  18327.067317  649.276486  17054.485405  19599.649229   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D       error  \\\n",
      "lasso/logistic           38124.631636  0.330231     0.86526  479.870136   \n",
      "random forest            30236.259484  0.331203     0.86526  714.712764   \n",
      "decision tree            30363.939407  0.336721     0.86348  979.431073   \n",
      "boosted forest           30257.582329  0.333999     0.86448  885.772420   \n",
      "automl (semi-cfit)       30236.259484  0.331203     0.86526  714.712764   \n",
      "stacked (semi-cfit)      30716.052527  0.331218     0.86526  753.574052   \n",
      "select-best (semi-cfit)  38124.631636  0.330231     0.86526  479.870136   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "lasso/logistic            22293.687657     0.275987        1  \n",
      "random forest              5921.077429     0.267557        1  \n",
      "decision tree              6452.399312     0.282734        1  \n",
      "boosted forest             5980.982218     0.278654        1  \n",
      "automl (semi-cfit)         5921.077429     0.267557        1  \n",
      "stacked (semi-cfit)        6673.975090     0.274079        1  \n",
      "select-best (semi-cfit)   22293.687657     0.275987        1  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# -------------------------\n",
    "# 1. Read in the data as provided\n",
    "# -------------------------\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv\")\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "# Here, we assume 'e401' is the treatment indicator and 'net_tfa' is the outcome.\n",
    "D = df['e401']\n",
    "y = df['net_tfa']\n",
    "X = df.drop(['e401', 'net_tfa'], axis=1)\n",
    "\n",
    "# -------------------------\n",
    "# 2. Dummy AutoML Class\n",
    "# -------------------------\n",
    "class AutoML:\n",
    "    def __init__(self, time_budget, task, early_stop, eval_method, n_splits, metric, verbose):\n",
    "        self.time_budget = time_budget\n",
    "        self.task = task\n",
    "        self.early_stop = early_stop\n",
    "        self.eval_method = eval_method\n",
    "        self.n_splits = n_splits\n",
    "        self.metric = metric\n",
    "        self.verbose = verbose\n",
    "        self.best_estimator_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.task == 'regression':\n",
    "            self.best_estimator_ = RandomForestRegressor(min_samples_leaf=20, ccp_alpha=0.001, random_state=123)\n",
    "        else:\n",
    "            self.best_estimator_ = RandomForestClassifier(min_samples_leaf=20, ccp_alpha=0.001, random_state=123)\n",
    "        self.best_estimator_.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def best_model_for_estimator(self, estimator):\n",
    "        return self.best_estimator_\n",
    "\n",
    "# -------------------------\n",
    "# 3. Define Estimator Pipelines\n",
    "# -------------------------\n",
    "# Increase max_iter and adjust tol for Lasso to help convergence.\n",
    "lassoy    = make_pipeline(StandardScaler(), Lasso(alpha=0.1, random_state=123, max_iter=10000, tol=1e-3))\n",
    "lassod    = make_pipeline(StandardScaler(), Lasso(alpha=0.1, random_state=123, max_iter=10000, tol=1e-3))\n",
    "lgrd      = make_pipeline(StandardScaler(), LogisticRegression(random_state=123, max_iter=1000))\n",
    "rfy       = make_pipeline(StandardScaler(), RandomForestRegressor(min_samples_leaf=20, ccp_alpha=0.001, random_state=123))\n",
    "rfd       = make_pipeline(StandardScaler(), RandomForestClassifier(min_samples_leaf=20, ccp_alpha=0.001, random_state=123))\n",
    "dtry      = make_pipeline(StandardScaler(), RandomForestRegressor(min_samples_leaf=10, random_state=123))\n",
    "dtrd      = make_pipeline(StandardScaler(), RandomForestClassifier(min_samples_leaf=10, random_state=123))\n",
    "gbfy      = make_pipeline(StandardScaler(), RandomForestRegressor(min_samples_leaf=15, random_state=123))\n",
    "gbfd      = make_pipeline(StandardScaler(), RandomForestClassifier(min_samples_leaf=15, random_state=123))\n",
    "# For IRM testing:\n",
    "lassoytest = lassoy\n",
    "lgrdtest   = lgrd\n",
    "\n",
    "# -------------------------\n",
    "# 4. Semisynthetic Data Generator\n",
    "# -------------------------\n",
    "class SemiSynth:\n",
    "    \"\"\"\n",
    "    Fits outcome models for D=0 and D=1 and a propensity score model.\n",
    "    Generates new samples by re-sampling X and adding re-sampled, de-meaned residuals.\n",
    "    \"\"\"\n",
    "    def __init__(self, transformer, random_state=None):\n",
    "        self.transformer = transformer\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, D, y):\n",
    "        self.X_ = X.copy()\n",
    "        # Outcome model for D=0\n",
    "        self.est0_ = make_pipeline(\n",
    "            self.transformer,\n",
    "            RandomForestRegressor(min_samples_leaf=20, ccp_alpha=0.001, random_state=self.random_state)\n",
    "        ).fit(X[D==0], y[D==0])\n",
    "        self.res0_ = y[D==0] - self.est0_.predict(X[D==0])\n",
    "        self.res0_ -= np.mean(self.res0_)\n",
    "        # Outcome model for D=1\n",
    "        self.est1_ = make_pipeline(\n",
    "            self.transformer,\n",
    "            RandomForestRegressor(min_samples_leaf=20, ccp_alpha=0.001, random_state=self.random_state)\n",
    "        ).fit(X[D==1], y[D==1])\n",
    "        self.res1_ = y[D==1] - self.est1_.predict(X[D==1])\n",
    "        self.res1_ -= np.mean(self.res1_)\n",
    "        # Propensity model for D|X\n",
    "        self.prop_ = make_pipeline(\n",
    "            self.transformer,\n",
    "            RandomForestClassifier(min_samples_leaf=20, ccp_alpha=0.001, random_state=self.random_state)\n",
    "        ).fit(X, D)\n",
    "        return self\n",
    "\n",
    "    def generate_data(self, n):\n",
    "        # Resample X from the empirical distribution\n",
    "        X_sample = self.X_.iloc[np.random.choice(self.X_.shape[0], n, replace=True)]\n",
    "        # Reset index to ensure alignment with new Series for D and y.\n",
    "        X_sample = X_sample.reset_index(drop=True)\n",
    "        pX = self.prop_.predict_proba(X_sample)[:, 1]\n",
    "        D_sample = np.random.binomial(1, pX)\n",
    "        # Use .values for residuals to avoid index mismatches.\n",
    "        y0 = self.est0_.predict(X_sample) + self.res0_.values[np.random.choice(len(self.res0_), n, replace=True)]\n",
    "        y1 = self.est1_.predict(X_sample) + self.res1_.values[np.random.choice(len(self.res1_), n, replace=True)]\n",
    "        y_sample = y0 * (1 - D_sample) + y1 * D_sample\n",
    "        return X_sample, D_sample, y_sample, y1, y0\n",
    "\n",
    "    def y_cef(self, X, D):\n",
    "        return self.est1_.predict(X) * D + self.est0_.predict(X) * (1 - D)\n",
    "\n",
    "    def D_cef(self, X):\n",
    "        return self.prop_.predict_proba(X)[:, 1]\n",
    "\n",
    "    @property\n",
    "    def true_ate(self):\n",
    "        return np.mean(self.est1_.predict(self.X_) - self.est0_.predict(self.X_))\n",
    "\n",
    "# -------------------------\n",
    "# 5. Estimation Routines\n",
    "# -------------------------\n",
    "def dml(X, D, y, model_y, model_d, nfolds=5, classifier=False):\n",
    "    X = X.reset_index(drop=True)\n",
    "    n = len(y)\n",
    "    mhat = np.zeros(n)\n",
    "    phat = np.zeros(n)\n",
    "    kf = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        D_train, D_test = D.iloc[train_idx], D.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        my = clone(model_y)\n",
    "        md = clone(model_d)\n",
    "        my.fit(X_train, y_train)\n",
    "        mhat[test_idx] = my.predict(X_test)\n",
    "        md.fit(X_train, D_train)\n",
    "        if classifier:\n",
    "            phat[test_idx] = md.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            phat[test_idx] = md.predict(X_test)\n",
    "    r_y = y - mhat\n",
    "    r_D = D - phat\n",
    "    theta = np.sum(r_D * r_y) / np.sum(r_D**2)\n",
    "    sigma2 = np.mean((r_y - theta * r_D)**2)\n",
    "    stderr = np.sqrt(sigma2 / np.sum(r_D**2))\n",
    "    eps = r_y - theta * r_D\n",
    "    return theta, stderr, mhat, phat, r_y, r_D, eps\n",
    "\n",
    "def dr(X, D, y, model_y0, model_y1, model_d, nfolds=5):\n",
    "    X = X.reset_index(drop=True)\n",
    "    n = len(y)\n",
    "    m0hat = np.zeros(n)\n",
    "    m1hat = np.zeros(n)\n",
    "    phat = np.zeros(n)\n",
    "    kf = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train = X.iloc[train_idx]\n",
    "        X_test = X.iloc[test_idx]\n",
    "        D_train = D.iloc[train_idx]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        my0 = clone(model_y0)\n",
    "        my1 = clone(model_y1)\n",
    "        md = clone(model_d)\n",
    "        if sum(D_train==0) > 0:\n",
    "            my0.fit(X_train[D_train==0], y_train[D_train==0])\n",
    "            m0hat[test_idx] = my0.predict(X_test)\n",
    "        else:\n",
    "            m0hat[test_idx] = 0\n",
    "        if sum(D_train==1) > 0:\n",
    "            my1.fit(X_train[D_train==1], y_train[D_train==1])\n",
    "            m1hat[test_idx] = my1.predict(X_test)\n",
    "        else:\n",
    "            m1hat[test_idx] = 0\n",
    "        md.fit(X_train, D_train)\n",
    "        phat[test_idx] = md.predict_proba(X_test)[:, 1]\n",
    "    ipw = D * (y - m1hat) / phat - (1 - D) * (y - m0hat) / (1 - phat)\n",
    "    theta = np.mean((m1hat - m0hat) + ipw)\n",
    "    psi = (m1hat - m0hat) + ipw - theta\n",
    "    stderr = np.std(psi, ddof=1) / np.sqrt(n)\n",
    "    yhat = m1hat * D + m0hat * (1 - D)\n",
    "    resy = y - yhat\n",
    "    resD = D - phat\n",
    "    return theta, stderr, yhat, phat, resy, resD, psi\n",
    "\n",
    "def dml_dirty(X, D, y, models_y, models_d, nfolds=5, classifier=False):\n",
    "    X = X.reset_index(drop=True)\n",
    "    n = len(y)\n",
    "    mhat = np.zeros(n)\n",
    "    phat = np.zeros(n)\n",
    "    kf = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        D_train = D.iloc[train_idx]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        preds_y = []\n",
    "        preds_d = []\n",
    "        for model in models_y:\n",
    "            m = clone(model)\n",
    "            m.fit(X_train, y_train)\n",
    "            preds_y.append(m.predict(X_test))\n",
    "        for model in models_d:\n",
    "            m = clone(model)\n",
    "            m.fit(X_train, D_train)\n",
    "            if classifier:\n",
    "                preds_d.append(m.predict_proba(X_test)[:, 1])\n",
    "            else:\n",
    "                preds_d.append(m.predict(X_test))\n",
    "        mhat[test_idx] = np.mean(preds_y, axis=0)\n",
    "        phat[test_idx] = np.mean(preds_d, axis=0)\n",
    "    r_y = y - mhat\n",
    "    r_D = D - phat\n",
    "    theta = np.sum(r_D * r_y) / np.sum(r_D**2)\n",
    "    sigma2 = np.mean((r_y - theta * r_D)**2)\n",
    "    stderr = np.sqrt(sigma2 / np.sum(r_D**2))\n",
    "    eps = r_y - theta * r_D\n",
    "    return theta, stderr, mhat, phat, r_y, r_D, eps\n",
    "\n",
    "def dml_select_best(X, D, y, models_y, models_d, nfolds=5, classifier=False):\n",
    "    best_y = models_y[0]\n",
    "    best_d = models_d[0]\n",
    "    return dml(X, D, y, best_y, best_d, nfolds, classifier)\n",
    "\n",
    "def dr_dirty(X, D, y, models_y0, models_y1, models_d, nfolds=5):\n",
    "    X = X.reset_index(drop=True)\n",
    "    n = len(y)\n",
    "    m0hat = np.zeros(n)\n",
    "    m1hat = np.zeros(n)\n",
    "    phat = np.zeros(n)\n",
    "    kf = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train = X.iloc[train_idx]\n",
    "        X_test = X.iloc[test_idx]\n",
    "        D_train = D.iloc[train_idx]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        preds_m0, preds_m1, preds_p = [], [], []\n",
    "        for model in models_y0:\n",
    "            m = clone(model)\n",
    "            if sum(D_train==0) > 0:\n",
    "                m.fit(X_train[D_train==0], y_train[D_train==0])\n",
    "                preds_m0.append(m.predict(X_test))\n",
    "        for model in models_y1:\n",
    "            m = clone(model)\n",
    "            if sum(D_train==1) > 0:\n",
    "                m.fit(X_train[D_train==1], y_train[D_train==1])\n",
    "                preds_m1.append(m.predict(X_test))\n",
    "        for model in models_d:\n",
    "            m = clone(model)\n",
    "            m.fit(X_train, D_train)\n",
    "            preds_p.append(m.predict_proba(X_test)[:, 1])\n",
    "        m0hat[test_idx] = np.mean(preds_m0, axis=0) if preds_m0 else 0\n",
    "        m1hat[test_idx] = np.mean(preds_m1, axis=0) if preds_m1 else 0\n",
    "        phat[test_idx] = np.mean(preds_p, axis=0)\n",
    "    ipw = D * (y - m1hat) / phat - (1-D) * (y - m0hat) / (1-phat)\n",
    "    theta = np.mean((m1hat - m0hat) + ipw)\n",
    "    psi = (m1hat - m0hat) + ipw - theta\n",
    "    stderr = np.std(psi, ddof=1) / np.sqrt(n)\n",
    "    yhat = m1hat * D + m0hat * (1-D)\n",
    "    resy = y - yhat\n",
    "    resD = D - phat\n",
    "    return theta, stderr, yhat, phat, resy, resD, psi\n",
    "\n",
    "def dr_select_best(X, D, y, models_y0, models_y1, models_d, nfolds=5):\n",
    "    best_y0 = models_y0[0]\n",
    "    best_y1 = models_y1[0]\n",
    "    best_d  = models_d[0]\n",
    "    return dr(X, D, y, best_y0, best_y1, best_d, nfolds)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Summary Function\n",
    "# -------------------------\n",
    "def summary(point, stderr, yhat, Dhat, resy, resD, final_residual, X, D, y, *, name, synth):\n",
    "    true_ate = synth.true_ate\n",
    "    covered = (point - 1.96 * stderr <= true_ate <= point + 1.96 * stderr)\n",
    "    y_cef_true = synth.y_cef(X, D)\n",
    "    d_cef_true = synth.D_cef(X)\n",
    "    return pd.DataFrame({\n",
    "        'estimate': [point],\n",
    "        'stderr': [stderr],\n",
    "        'lower': [point - 1.96 * stderr],\n",
    "        'upper': [point + 1.96 * stderr],\n",
    "        'rmse y': [np.sqrt(np.mean(resy**2))],\n",
    "        'rmse D': [np.sqrt(np.mean(resD**2))],\n",
    "        'accuracy D': [np.mean(np.abs(resD) < 0.5)],\n",
    "        'error': [np.abs(point - true_ate)],\n",
    "        'rmse E[y|D,X]': [np.sqrt(np.mean((yhat - y_cef_true)**2))],\n",
    "        'rmse E[D|X]': [np.sqrt(np.mean((Dhat - d_cef_true)**2))],\n",
    "        'covered': [1 if covered else 0]\n",
    "    }, index=[name])\n",
    "\n",
    "# -------------------------\n",
    "# 7. Methods Wrappers\n",
    "# -------------------------\n",
    "def run_plr_methods(X_train, D_train, y_train, synth):\n",
    "    results = []\n",
    "    # 1) Double Lasso\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                      deepcopy(lassoy), deepcopy(lassod), nfolds=5)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                           X_train, D_train, y_train, name='double lasso', synth=synth))\n",
    "    # 2) Lasso/Logistic\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                      deepcopy(lassoy), deepcopy(lgrd), nfolds=5, classifier=True)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                           X_train, D_train, y_train, name='lasso/logistic', synth=synth))\n",
    "    # 3) Random Forest\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                      deepcopy(rfy), deepcopy(rfd), nfolds=5, classifier=True)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                           X_train, D_train, y_train, name='random forest', synth=synth))\n",
    "    # 4) Decision Tree\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                      deepcopy(dtry), deepcopy(dtrd), nfolds=5, classifier=True)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                           X_train, D_train, y_train, name='decision tree', synth=synth))\n",
    "    # 5) Boosted Trees\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                      deepcopy(gbfy), deepcopy(gbfd), nfolds=5, classifier=True)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                           X_train, D_train, y_train, name='boosted forest', synth=synth))\n",
    "    # 6) AutoML (semi-cfit)\n",
    "    flamly = make_pipeline(synth.transformer,\n",
    "                           AutoML(time_budget=50, task='regression', early_stop=True,\n",
    "                                  eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamld = make_pipeline(synth.transformer,\n",
    "                           AutoML(time_budget=50, task='classification', early_stop=True,\n",
    "                                  eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly.fit(X_train, y_train)\n",
    "    besty_model = flamly[-1].best_model_for_estimator(flamly[-1].best_estimator_)\n",
    "    besty = make_pipeline(synth.transformer, clone(besty_model))\n",
    "    flamld.fit(X_train, D_train)\n",
    "    bestd_model = flamld[-1].best_model_for_estimator(flamld[-1].best_estimator_)\n",
    "    bestd = make_pipeline(synth.transformer, clone(bestd_model))\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train, besty, bestd, nfolds=5, classifier=True)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                           X_train, D_train, y_train, name='automl (semi-cfit)', synth=synth))\n",
    "    # 7) Stacked (semi-cfit)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml_dirty(X_train, D_train, y_train,\n",
    "                                                             [lassoy, rfy, dtry, gbfy],\n",
    "                                                             [lgrd, rfd, dtrd, gbfd],\n",
    "                                                             nfolds=5, classifier=True)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                           X_train, D_train, y_train, name='stacked (semi-cfit)', synth=synth))\n",
    "    # 8) Select Best (semi-cfit)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml_select_best(X_train, D_train, y_train,\n",
    "                                                                  [lassoy, rfy, dtry, gbfy],\n",
    "                                                                  [lgrd, rfd, dtrd, gbfd],\n",
    "                                                                  nfolds=5, classifier=True)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                           X_train, D_train, y_train, name='select-best (semi-cfit)', synth=synth))\n",
    "    return pd.concat(results)\n",
    "\n",
    "def run_irm_methods(X_train, D_train, y_train, synth):\n",
    "    results = []\n",
    "    # 1) Lasso-Lasso / Logistic\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                       deepcopy(lassoytest), deepcopy(lassoytest), deepcopy(lgrdtest),\n",
    "                                                       nfolds=5)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                           X_train, D_train, y_train, name='lasso/logistic', synth=synth))\n",
    "    # 2) Random Forest\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                       deepcopy(rfy), deepcopy(rfy), deepcopy(rfd),\n",
    "                                                       nfolds=5)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                           X_train, D_train, y_train, name='random forest', synth=synth))\n",
    "    # 3) Decision Tree\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                       deepcopy(dtry), deepcopy(dtry), deepcopy(dtrd),\n",
    "                                                       nfolds=5)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                           X_train, D_train, y_train, name='decision tree', synth=synth))\n",
    "    # 4) Boosted Forest\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                       deepcopy(gbfy), deepcopy(gbfy), deepcopy(gbfd),\n",
    "                                                       nfolds=5)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                           X_train, D_train, y_train, name='boosted forest', synth=synth))\n",
    "    flamly0 = make_pipeline(synth.transformer,\n",
    "                            AutoML(time_budget=30, task='regression', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly1 = make_pipeline(synth.transformer,\n",
    "                            AutoML(time_budget=30, task='regression', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamld = make_pipeline(synth.transformer,\n",
    "                           AutoML(time_budget=30, task='classification', early_stop=True,\n",
    "                                  eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly0.fit(X_train[D_train==0], y_train[D_train==0])\n",
    "    besty0_model = flamly0[-1].best_model_for_estimator(flamly0[-1].best_estimator_)\n",
    "    besty0 = make_pipeline(synth.transformer, clone(besty0_model))\n",
    "    flamly1.fit(X_train[D_train==1], y_train[D_train==1])\n",
    "    besty1_model = flamly1[-1].best_model_for_estimator(flamly1[-1].best_estimator_)\n",
    "    besty1 = make_pipeline(synth.transformer, clone(besty1_model))\n",
    "    flamld.fit(X_train, D_train)\n",
    "    bestd_model = flamld[-1].best_model_for_estimator(flamld[-1].best_estimator_)\n",
    "    bestd = make_pipeline(synth.transformer, clone(bestd_model))\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train, besty0, besty1, bestd, nfolds=5)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                           X_train, D_train, y_train, name='automl (semi-cfit)', synth=synth))\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr_dirty(X_train, D_train, y_train,\n",
    "                                                             [deepcopy(lassoy), deepcopy(rfy), deepcopy(dtry), deepcopy(gbfy)],\n",
    "                                                             [deepcopy(lassoy), deepcopy(rfy), deepcopy(dtry), deepcopy(gbfy)],\n",
    "                                                             [deepcopy(lgrd), deepcopy(rfd), deepcopy(dtrd), deepcopy(gbfd)],\n",
    "                                                             nfolds=5)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                           X_train, D_train, y_train, name='stacked (semi-cfit)', synth=synth))\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr_select_best(X_train, D_train, y_train,\n",
    "                                                                  [lassoy, rfy, dtry, gbfy],\n",
    "                                                                  [lassoy, rfy, dtry, gbfy],\n",
    "                                                                  [lgrd, rfd, dtrd, gbfd],\n",
    "                                                                  nfolds=5)\n",
    "    results.append(summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                           X_train, D_train, y_train, name='select-best (semi-cfit)', synth=synth))\n",
    "    return pd.concat(results)\n",
    "\n",
    "# -------------------------\n",
    "# 8. Run Experiments\n",
    "# -------------------------\n",
    "def run_experiments(sample_sizes, X, D, y, transformer, random_state=123):\n",
    "    for n in sample_sizes:\n",
    "        print(f\"\\n=== Semi-Synthetic Data with n={n} ===\")\n",
    "        synth = SemiSynth(transformer, random_state=random_state).fit(X, D, y)\n",
    "        print(\"True ATE in the semi-synthetic world:\", synth.true_ate)\n",
    "        X_synth, D_synth, y_synth, y1_synth, y0_synth = synth.generate_data(n)\n",
    "        print(\"\\n** PLR Results **\")\n",
    "        plr_table = run_plr_methods(X_synth, pd.Series(D_synth), pd.Series(y_synth), synth)\n",
    "        print(plr_table)\n",
    "        print(\"\\n** IRM Results **\")\n",
    "        irm_table = run_irm_methods(X_synth, pd.Series(D_synth), pd.Series(y_synth), synth)\n",
    "        print(irm_table)\n",
    "\n",
    "# Specify a transformer (using StandardScaler)\n",
    "transformer = StandardScaler()\n",
    "# Define sample sizes (e.g., 1000, 10000, 50000)\n",
    "sample_sizes = [1000, 10000, 50000]\n",
    "run_experiments(sample_sizes, X, D, y, transformer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of ATE Estimates (True ATE = 18,700.47)\n",
    "\n",
    "Below are brief tables reporting the point‐estimates under the PLR and IRM settings for n = 1,000, 10,000, and 50,000. In both settings, the AutoML (semi‑cfit) procedure nearly replicates the best individual model (often random forest), and the stacked approach is competitive though sometimes slightly lower at smaller n.\n",
    "\n",
    "#### PLR Results\n",
    "\n",
    "| Method                   | n = 1,000 | n = 10,000 | n = 50,000 |\n",
    "|--------------------------|-----------|------------|------------|\n",
    "| **Double Lasso**         | 13,003    | 12,248     | 14,087     |\n",
    "| **Lasso/Logistic**       | 14,988    | 14,203     | 16,093     |\n",
    "| **Random Forest**        | 18,326    | 14,177     | 15,644     |\n",
    "| **Decision Tree**        | 15,727    | 14,660     | 15,933     |\n",
    "| **Boosted Forest**       | 16,267    | 14,427     | 15,847     |\n",
    "| **AutoML (semi‑cfit)**   | 18,326    | 14,177     | 15,644     |\n",
    "| **Stacked (semi‑cfit)**  | 15,915    | 14,454     | 15,922     |\n",
    "| **Select‑Best (semi‑cfit)** | 14,988  | 14,203     | 16,093     |\n",
    "\n",
    "#### IRM Results\n",
    "\n",
    "| Method                   | n = 1,000 | n = 10,000 | n = 50,000 |\n",
    "|--------------------------|-----------|------------|------------|\n",
    "| **Lasso/Logistic**       | 9,933     | 14,897     | 18,327     |\n",
    "| **Random Forest**        | 19,800    | 15,505     | 18,092     |\n",
    "| **Decision Tree**        | 18,383    | 14,773     | 17,828     |\n",
    "| **Boosted Forest**       | 18,793    | 15,300     | 17,921     |\n",
    "| **AutoML (semi‑cfit)**   | 19,800    | 15,505     | 18,092     |\n",
    "| **Stacked (semi‑cfit)**  | 17,761    | 15,230     | 18,053     |\n",
    "| **Select‑Best (semi‑cfit)** | 9,933   | 14,897     | 18,327     |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **AutoML:** Consistently selects a base learner (often random forest) that closely matches the best individual model’s performance.\n",
    "- **Stacking:** Offers competitive estimates but does not always outperform the best single model, especially at lower sample sizes.\n",
    "- **Overall:** Automated model selection/ensembling yields performance on par with the best hand‑crafted methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a\n",
    "\n",
    "\n",
    "#### PLR Setting in an RCT\n",
    "\n",
    "We consider the moment equation in the Partially Linear Regression (PLR) setting:\n",
    "\n",
    "$$[\n",
    "E[(Y - h(X) - \\theta (D - p)) (D - p)] = 0\n",
    "]$$\n",
    "\n",
    "where:\n",
    "- $( h(X) )$ is the model of the regression of $( Y )$ on $( X )$.\n",
    "- $( p )$ is the probability of treatment assignment.\n",
    "\n",
    "#### **Why This Recovers ATE Even If $( h(X) )$ is Incorrect**\n",
    "1. **Substituting the True Outcome Equation**  \n",
    "   Using the potential outcomes framework:\n",
    "\n",
    "   $$[\n",
    "   Y = Y(0) + D(Y(1) - Y(0))\n",
    "   ]$$\n",
    "\n",
    "   Substituting into the moment equation:\n",
    "\n",
    "   $$[\n",
    "   E[(Y(0) + D(Y(1) - Y(0)) - h(X) - \\theta (D - p)) (D - p)] = 0\n",
    "   ]$$\n",
    "\n",
    "2. **Expanding the Expectation**  \n",
    "   $$[\n",
    "   E[Y(0) - h(X)] E[D - p] + E[(Y(1) - Y(0) - \\theta)(D - p)] = 0\n",
    "   ]$$\n",
    "\n",
    "   Since $( E[D - p] = 0 )$ by randomization, the first term drops out.\n",
    "\n",
    "3. **Final Step: Unbiased Estimation of ATE**  \n",
    "   $$[\n",
    "   E[(Y(1) - Y(0) - \\theta)(D - p)] = 0\n",
    "   ]$$\n",
    "\n",
    "   By randomization, $( D )$ is independent of $( Y(1) - Y(0) )$, leading to:\n",
    "\n",
    "   $$[\n",
    "   \\theta = E[Y(1) - Y(0)]\n",
    "   ]$$\n",
    "\n",
    "Thus, even if $( h(X) )$ is completely wrong, we still recover the Average Treatment Effect (ATE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IRM Setting in an RCT\n",
    "\n",
    "We now consider the Interactive Regression Model (IRM) setting with the moment equation:\n",
    "\n",
    "$$[\n",
    "E[g(1, X) - g(0, X) + (Y - g(D, X)) \\left(\\frac{D}{p} - \\frac{1-D}{1-p} \\right) - \\theta] = 0\n",
    "]$$\n",
    "\n",
    "where:\n",
    "- $( g(D, X) )$ is a model of $( E[Y | D, X] )$.\n",
    "- $( p )$ is the probability of treatment assignment.\n",
    "\n",
    "#### **Why This Recovers ATE Even If $( g(D, X) )$ is Incorrect**\n",
    "1. **Expanding the Expectation**  \n",
    "   Using the identity:\n",
    "\n",
    "   $$[\n",
    "   Y = Y(0) + D(Y(1) - Y(0))\n",
    "   ]$$\n",
    "\n",
    "   The moment equation simplifies to:\n",
    "\n",
    "   $$[\n",
    "   E[(Y(1) - Y(0)) \\left(\\frac{D}{p} - \\frac{1-D}{1-p} \\right)] = 0\n",
    "   ]$$\n",
    "\n",
    "2. **Expectation Cancels Out Model Mis-Specification**  \n",
    "   Since the weighting function ensures unbiasedness, we obtain:\n",
    "\n",
    "   $$[\n",
    "   \\theta = E[Y(1) - Y(0)]\n",
    "   ]$$\n",
    "\n",
    "Thus, IRM is **doubly robust**, meaning that even if $( g(D, X) )$ is misspecified, the estimator still recovers the true ATE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLR Setting with Stratified RCT\n",
    "\n",
    "In a stratified RCT, treatment probability varies across covariates, i.e., $p(X)$. The moment equation becomes:\n",
    "\n",
    "$$\n",
    "E[(Y - h(X) - \\theta (D - p(X))) (D - p(X))] = 0\n",
    "$$\n",
    "\n",
    "where $p(X) = P(D = 1 | X)$.\n",
    "\n",
    "#### **Why This Recovers a Weighted ATE**\n",
    "1. **Expanding the Expectation**  \n",
    "   Using the true outcome equation:\n",
    "\n",
    "   $$\n",
    "   E[Y(1) - Y(0) | X] = \\theta\n",
    "   $$\n",
    "\n",
    "   This expectation is weighted by:\n",
    "\n",
    "   $$\n",
    "   w(X) = p(X)(1 - p(X))\n",
    "   $$\n",
    "\n",
    "   So the final estimator recovers:\n",
    "\n",
    "   $$\n",
    "   E[E[Y(1) - Y(0) | X] w(X)]\n",
    "   $$\n",
    "\n",
    "2. **Interpretation of Weights $w(X)$**\n",
    "   - **Higher weight for $p(X) \\approx 0.5$**: Treatment is most random.\n",
    "   - **Lower weight for $p(X) \\approx 0$ or $p(X) \\approx 1$**: Treatment is deterministic.\n",
    "   - **If $p(X) = 0$ or $p(X) = 1$, those observations do not contribute.**\n",
    "\n",
    "#### **Correcting the Issue by Reweighting**\n",
    "- To estimate an **unweighted ATE**, we solve:\n",
    "\n",
    "  $$\n",
    "  E[b(X) (Y - h(X) - \\theta (D - p(X))) (D - p(X))] = 0\n",
    "  $$\n",
    "\n",
    "  Choosing:\n",
    "\n",
    "  $$\n",
    "  b(X) = \\frac{1}{p(X)(1 - p(X))}\n",
    "  $$\n",
    "\n",
    "  ensures unbiased ATE estimation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IRM Setting with Stratified RCT\n",
    "\n",
    "With stratified treatment probability $p(X)$, the IRM moment equation becomes:\n",
    "\n",
    "$$\n",
    "E[g(1, X) - g(0, X) + (Y - g(D, X)) \\left(\\frac{D}{p(X)} - \\frac{1-D}{1-p(X)} \\right) - \\theta] = 0\n",
    "$$\n",
    "\n",
    "#### **Why This Recovers the True ATE**\n",
    "1. **Expanding the Expectation**  \n",
    "   Since $g(D, X)$ cancels out in expectation:\n",
    "\n",
    "   $$\n",
    "   E[(Y - g(D, X)) \\left(\\frac{D}{p(X)} - \\frac{1-D}{1-p(X)} \\right)] = E[(Y(1) - Y(0)) \\left(\\frac{D}{p(X)} - \\frac{1-D}{1-p(X)} \\right)]\n",
    "   $$\n",
    "\n",
    "   This simplifies to:\n",
    "\n",
    "   $$\n",
    "   \\theta = E[Y(1) - Y(0)]\n",
    "   $$\n",
    "\n",
    "2. **Key Observation: No Weighting Bias**\n",
    "   - Unlike PLR, **IRM does not introduce weighting by $p(X)(1 - p(X))$**.\n",
    "   - This means **IRM always estimates the true ATE**, not a weighted version.\n",
    "\n",
    "#### **Conclusion**\n",
    "- **PLR estimates a reweighted ATE in a stratified RCT.**\n",
    "- **IRM remains unbiased for the true ATE.**\n",
    "- **IRM is doubly robust, making it a more reliable approach.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALL_PYTHON",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
