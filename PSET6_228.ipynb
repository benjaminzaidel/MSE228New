{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSET 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINK: https://github.com/benjaminzaidel/MSE228New/blob/master/PSET6_228.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)\n",
    "\n",
    "What was the estimate of the causal effect of the treatment and what is the 95% confidence interval? The second OLS model predicts the outcome from just the controls. Which control factors were statistically significant at the 95% level? The third OLS model predicts treatment from the controls. Which control factors were statistically significant at the 95% level? Subsequently, we estimate the same treatment effect using double lasso, and control for third degree polynomials. What was the estimate and 95% percent confidence interval from the double lasso, and compare your results with that from the OLS model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"color: red\"> \n",
    "\n",
    "\n",
    "Estimate of causal effect of treatment based on colab: **0.0973**\n",
    "\n",
    "95% confidence interval: **[0.051,0.144]**\n",
    "\n",
    "Statistically significant control factors in second model: **femaleR, ageR**\n",
    "\n",
    "Statistically significant control factors in THIRD model: **herderR**\n",
    "\n",
    "Causal effect estimate for double lasso: **0.1003**\n",
    "\n",
    "95% confidence interval: **[0.052,0.148]**\n",
    "\n",
    "We see very similar results between the OLS model and the double lasso model (almost the exact same numbers). This suggests that OLS was already controlling for key confounders, meaning no major omitted variable bias. This also indicates that high-dimensional interactions or nonlinearities didn’t significantly impact the treatment effect, and the results are robust across methods. Since the confidence intervals are also similar, both methods provide consistent and stable estimates, increasing confidence in the causal effect.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "\n",
    "The next part of the notebook analyzes the bias from unobserved confounding. The analyst needs to provide a partial R-square for both the outcome and treatment that can be further explained by some unobserved confounding. Replace the current R-square in the notebook with the R-squares from the OLS models. In particular, the R-square for the outcome Y should be set to the R-square from the OLS predicting the outcome just from the controls. Similarly, the R-square for the treatment should be set to the R-square from the OLS predicting the treatment just from the controls.This way we are positing that the unobserved confounders are as strong of a control as the controls that we already have. What is the bias that such an unobserved confounder can cause, and is it sufficient to overturn the treatment finding from the double lasso, i.e. if we subtract this bias from the lower end of the CI, will we get a non-positive treatment effect? Then we plot the level set of the bias, as we vary the R-square of the treatment and the R-square of the outcome. Looking at the plot, if the R-square for the treatment that the unobserved confounder further explains was 0.1, approximately how much should the R-square of the outcome be, so that the unobserved confounder can lead to the same amount of bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main estimate\n",
    "beta = dml_model.params[1]\n",
    "\n",
    "# Hypothetical values of partial R2s\n",
    "R2_YC = 0.135\n",
    "R2_DC = 0.018\n",
    "\n",
    "# Elements of the bias equation\n",
    "kappa = (R2_YC * R2_DC) / (1 - R2_DC)\n",
    "variance_ratio = np.mean(dml_model.resid**2) / np.mean(resD**2)\n",
    "\n",
    "# Compute square bias\n",
    "BiasSq = kappa * variance_ratio\n",
    "\n",
    "# Compute absolute value of the bias\n",
    "print(\"absolute value of the bias:\", np.sqrt(BiasSq))\n",
    "\n",
    "# Plotting\n",
    "gridR2_DC = np.arange(0, 0.301, 0.001)\n",
    "gridR2_YC = kappa * (1 - gridR2_DC) / gridR2_DC\n",
    "gridR2_YC = np.where(gridR2_YC > 1, 1, gridR2_YC)\n",
    "\n",
    "#Extra print line\n",
    "print(\"Curve value at x = 0.1:\", gridR2_YC[np.where(gridR2_DC == 0.1)][0])\n",
    "\n",
    "plt.plot(gridR2_DC, gridR2_YC, color='red')\n",
    "plt.xlabel('Partial R2 of Treatment with Confounder')\n",
    "plt.ylabel('Partial R2 of Outcome with Confounder')\n",
    "plt.title(f'Combo of R2 such that |Bias| < {np.round(np.sqrt(BiasSq), decimals=4)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"color: red\"> \n",
    "\n",
    "Absolute value of the bias: **0.032444448889241514** \n",
    "\n",
    "Curve value at x = 0.1: 0.022270875763747457\n",
    "\n",
    "Even after we subtract this value from the lower bound of confidence interval [0.052,0.148], we get [0.01955555111, 0.148]. As this is >0, there is no need to overturn the treatment finding.\n",
    "\n",
    "Finally, given our point (0.1, 0.022270875763747457), we can conclude that if the unobserved confounder explained 10% of the variation in the treatment, it would only need to explain ~2.23% of the variation in the outcome to produce the same level of bias.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](outputpartb.png \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)\n",
    "\n",
    "The rest of the code packages everything into a set of functions that captures these functionalities, but also captures the uncertainty in the calculation of the bias. For instance, dml_sensitivity_bounds produces upper and lower bounds as a function of the partial R-square of the outcome and the partial R-square of the treatment. Call this function to get sensitivity bound for the same positive R-squared values as you did in the previous calculations, then call it again and incorporate the uncertainty from the bias calculation with 95% confidence. Does incorporating this extra uncertainty overturn the results? Keeping the R-square of the outcome the same, what is the smallest R-square of the treatment needed to overturn the treatment effect estimation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_uncertainty = dml_sensitivity_bounds((resY, resD), R2_YC, R2_DC)\n",
    "print(no_uncertainty)\n",
    "\n",
    "uncertainty = dml_sensitivity_bounds((resY, resD), R2_YC, R2_DC, alpha=0.05)\n",
    "print(uncertainty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With no uncertainty: (0.0678919992440549, 0.13278089702253792)\n",
    "\n",
    "With uncertainty (alpha = 0.05): (0.023034680259483034, 0.1775960026935742)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_uncertainty = dml_sensitivity_bounds((resY, resD), R2_YC, R2_DC)\n",
    "print(no_uncertainty)\n",
    "\n",
    "uncertainty = dml_sensitivity_bounds((resY, resD), R2_YC, R2_DC, alpha=0.05)\n",
    "print(uncertainty)\n",
    "\n",
    "R2_DC_max = 0.018\n",
    "while dml_sensitivity_bounds((resY, resD), R2_YC, R2_DC_max, alpha=0.05)[0]>0:\n",
    "    R2_DC_max += 0.0001\n",
    "print(R2_DC_max)\n",
    "print(dml_sensitivity_bounds((resY, resD), R2_YC, R2_DC_max, alpha=0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.0678919992440549, 0.13278089702253792)\n",
    "(0.023034680259483034, 0.1775960026935742)\n",
    "0.05090000000000048\n",
    "(-1.701343652082965e-05, 0.20064798164717013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with uncertainty, lower bound remains positive, showing that uncertainty does not overturn the results. \n",
    "\n",
    "Highest R2_DC_max (R^2 of treatment) you can set before results get overturned is ~0.0509."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) \n",
    "\n",
    "Repeat the analysis in that notebook for the low income (bottom 25% of the income distribution) and for the high income groups (top 25% of the income distribution). Report the tables with the results under the PLIV and the corresponding interactive IV (LATE) model assumptions. Comment on whether there is any heterogeneity on the effect of 401k with respect to income. Comment on whether the different ML methods provide consistent estimates with each other, for each of these income groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LassoCV, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (GradientBoostingRegressor, GradientBoostingClassifier,\n",
    "                              RandomForestRegressor, RandomForestClassifier)\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator, clone, TransformerMixin\n",
    "import scipy.stats\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Load data and define groups\n",
    "file = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv\"\n",
    "data_all = pd.read_csv(file)\n",
    "data_bottom = data_all.loc[data_all['inc'] < data_all['inc'].quantile(0.25)].copy()\n",
    "data_top = data_all.loc[data_all['inc'] > data_all['inc'].quantile(0.75)].copy()\n",
    "\n",
    "# A simple transformer using a formula (returns a numpy array)\n",
    "from formulaic import Formula\n",
    "class FormulaTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, formula, array=False):\n",
    "        self.formula = formula\n",
    "        self.array = array\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        df = Formula(self.formula).get_model_matrix(X)\n",
    "        return df.values if self.array else df\n",
    "\n",
    "transformer = FormulaTransformer(\n",
    "    \"0 + poly(age, degree=6, raw=True) + poly(inc, degree=8, raw=True) \"\n",
    "    \"+ poly(educ, degree=4, raw=True) + poly(fsize, degree=2, raw=True) \"\n",
    "    \"+ male + marr + twoearn + db + pira + hown\", array=True)\n",
    "\n",
    "# Columns to drop from the dataset when forming controls X\n",
    "drop_cols = ['e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "             'hval', 'hmort', 'hequity',\n",
    "             'nifa', 'net_nifa', 'net_n401', 'ira',\n",
    "             'dum91', 'icat', 'ecat', 'zhat',\n",
    "             'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7',\n",
    "             'a1', 'a2', 'a3', 'a4', 'a5']\n",
    "\n",
    "\n",
    "class TransformedEstimator(BaseEstimator):\n",
    "    def __init__(self, transformer, estimator):\n",
    "        self.transformer = transformer\n",
    "        self.estimator = estimator\n",
    "    def fit(self, X, y):\n",
    "        X_trans = self.transformer.fit_transform(X)\n",
    "        self.estimator.fit(X_trans, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        X_trans = self.transformer.transform(X)\n",
    "        return self.estimator.predict(X_trans)\n",
    "    def predict_proba(self, X):\n",
    "        X_trans = self.transformer.transform(X)\n",
    "        return self.estimator.predict_proba(X_trans)\n",
    "\n",
    "\n",
    "# Functions for PLIV Analysis\n",
    "\n",
    "def dml(X, Z, D, y, modely, modeld, modelz, *, nfolds, classifier=False):\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhat = cross_val_predict(modely, X, y, cv=cv, n_jobs=-1)\n",
    "    if classifier:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "        Zhat = cross_val_predict(modelz, X, Z, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    else:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1)\n",
    "        Zhat = cross_val_predict(modelz, X, Z, cv=cv, n_jobs=-1)\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "    resZ = Z - Zhat\n",
    "    point = np.mean(resy * resZ) / np.mean(resD * resZ)\n",
    "    epsilon = resy - point * resD\n",
    "    var = np.mean(epsilon**2 * resZ**2) / np.mean(resD * resZ)**2\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, epsilon\n",
    "\n",
    "def summary(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, epsilon, X, Z, D, y, *, name):\n",
    "    return pd.DataFrame({\n",
    "        'estimate': point,\n",
    "        'stderr': stderr,\n",
    "        'lower': point - 1.96 * stderr,\n",
    "        'upper': point + 1.96 * stderr,\n",
    "        'rmse y': np.sqrt(np.mean(resy**2)),\n",
    "        'rmse D': np.sqrt(np.mean(resD**2)),\n",
    "        'rmse Z': np.sqrt(np.mean(resZ**2)),\n",
    "        'accuracy D': np.mean(np.abs(resD) < 0.5),\n",
    "        'accuracy Z': np.mean(np.abs(resZ) < 0.5)\n",
    "    }, index=[name])\n",
    "\n",
    "def robust_inference(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, epsilon, X, Z, D, y, *, grid, alpha=0.05):\n",
    "    n = X.shape[0]\n",
    "    thr = scipy.stats.chi2.ppf(1 - alpha, df=1)\n",
    "    accept = []\n",
    "    for theta in grid:\n",
    "        moment = (resy - theta * resD) * resZ\n",
    "        test = n * np.mean(moment)**2 / np.var(moment)\n",
    "        if test <= thr:\n",
    "            accept.append(theta)\n",
    "    return accept\n",
    "\n",
    "def run_pliv_analysis(data, desc, transformer):\n",
    "    print(f\"---------- PLIV Analysis for {desc} ----------\")\n",
    "    y = data['net_tfa'].values\n",
    "    Z = data['e401'].values\n",
    "    D = data['p401'].values\n",
    "    X = data.drop(drop_cols, axis=1)\n",
    "    \n",
    "    results = []\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    # Method 1: Double Lasso\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lassod = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lassoz = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    res = dml(X, Z, D, y, lassoy, lassod, lassoz, nfolds=3)\n",
    "    results.append(summary(*res, X, Z, D, y, name='double lasso'))\n",
    "    \n",
    "    # Method 2: Lasso + Logistic Regression\n",
    "    lgrd = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    lgrz = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    res = dml(X, Z, D, y, lassoy, lgrd, lgrz, nfolds=3, classifier=True)\n",
    "    results.append(summary(*res, X, Z, D, y, name='lasso/logistic'))\n",
    "    \n",
    "    # Method 3: Random Forests\n",
    "    rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001))\n",
    "    rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001))\n",
    "    rfz = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001))\n",
    "    res = dml(X, Z, D, y, rfy, rfd, rfz, nfolds=3, classifier=True)\n",
    "    results.append(summary(*res, X, Z, D, y, name='random forest'))\n",
    "    \n",
    "    # Method 4: Decision Trees\n",
    "    dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=0.001))\n",
    "    dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=0.001))\n",
    "    dtrz = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=0.001))\n",
    "    res = dml(X, Z, D, y, dtry, dtrd, dtrz, nfolds=3, classifier=True)\n",
    "    results.append(summary(*res, X, Z, D, y, name='decision tree'))\n",
    "    \n",
    "    # Method 5: Boosted Trees\n",
    "    gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "    gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    gbfz = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    res = dml(X, Z, D, y, gbfy, gbfd, gbfz, nfolds=3, classifier=True)\n",
    "    results.append(summary(*res, X, Z, D, y, name='boosted forest'))\n",
    "    \n",
    "    # Method 6: AutoML via FLAML (try/except to bypass if errors occur)\n",
    "    try:\n",
    "        from flaml import AutoML\n",
    "        X_trans = transformer.fit_transform(X)\n",
    "        automl_reg = AutoML(time_budget=100, task='regression', early_stop=5, max_iter=100,\n",
    "                            eval_method='cv', n_splits=3, metric='r2', verbose=0)\n",
    "        automl_reg.fit(X_trans, y)\n",
    "        best_reg = automl_reg.best_estimator\n",
    "        automl_clf = AutoML(time_budget=100, task='classification', early_stop=5, max_iter=100,\n",
    "                            eval_method='cv', n_splits=3, metric='r2', verbose=0)\n",
    "        automl_clf.fit(X_trans, D)\n",
    "        best_d = automl_clf.best_estimator\n",
    "        automl_clf.fit(X_trans, Z)\n",
    "        best_z = automl_clf.best_estimator\n",
    "        modely_auto = TransformedEstimator(transformer, clone(best_reg))\n",
    "        modeld_auto = TransformedEstimator(transformer, clone(best_d))\n",
    "        modelz_auto = TransformedEstimator(transformer, clone(best_z))\n",
    "        res_auto = dml(X, Z, D, y, modely_auto, modeld_auto, modelz_auto, nfolds=3, classifier=True)\n",
    "        results.append(summary(*res_auto, X, Z, D, y, name='automl (semi-cfit)'))\n",
    "    except Exception as e:\n",
    "        print(\"FLAML AutoML failed, skipping AutoML method. Error:\", e)\n",
    "    \n",
    "    table_pliv = pd.concat(results)\n",
    "    print(table_pliv)\n",
    "    \n",
    "    region = robust_inference(*res, X, Z, D, y, grid=np.linspace(0, 20000, 10000))\n",
    "    beta = np.mean(res[7] * res[2]) / np.mean(res[6] * res[2])\n",
    "    var_beta = np.mean((res[6] - beta * res[7])**2 * res[2]**2) / np.mean(res[2]**2)**2\n",
    "    se_beta = np.sqrt(var_beta / res[2].shape[0])\n",
    "    t_stat = np.abs(beta / se_beta)\n",
    "    \n",
    "    print(f\"Robust PLIV region: {np.min(region)} to {np.max(region)}\")\n",
    "    print(f\"PLIV t-statistic (approx.): {t_stat:.3f}\\n\")\n",
    "    \n",
    "    return table_pliv\n",
    "\n",
    "\n",
    "# Functions for IIV Analysis (similar try/except for FLAML part)\n",
    "\n",
    "def iiv(X, Z, D, y, modely0, modely1, modeld0, modeld1, modelz, *, nfolds, trimming=0.01):\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhat0, yhat1 = np.zeros(y.shape), np.zeros(y.shape)\n",
    "    Dhat0, Dhat1 = np.zeros(D.shape), np.zeros(D.shape)\n",
    "    for train, test in cv.split(X, y):\n",
    "        yhat0[test] = clone(modely0).fit(X.iloc[train][Z[train] == 0], y[train][Z[train] == 0]).predict(X.iloc[test])\n",
    "        yhat1[test] = clone(modely1).fit(X.iloc[train][Z[train] == 1], y[train][Z[train] == 1]).predict(X.iloc[test])\n",
    "        if np.mean(D[train][Z[train] == 0]) > 0:\n",
    "            modeld0_ = clone(modeld0).fit(X.iloc[train][Z[train] == 0], D[train][Z[train] == 0])\n",
    "            Dhat0[test] = modeld0_.predict_proba(X.iloc[test])[:, 1]\n",
    "        if np.mean(D[train][Z[train] == 1]) < 1:\n",
    "            modeld1_ = clone(modeld1).fit(X.iloc[train][Z[train] == 1], D[train][Z[train] == 1])\n",
    "            Dhat1[test] = modeld1_.predict_proba(X.iloc[test])[:, 1]\n",
    "        else:\n",
    "            Dhat1[test] = 1\n",
    "    yhat = yhat0 * (1 - Z) + yhat1 * Z\n",
    "    Dhat = Dhat0 * (1 - Z) + Dhat1 * Z\n",
    "    Zhat = cross_val_predict(modelz, X, Z, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    Zhat = np.clip(Zhat, trimming, 1 - trimming)\n",
    "    HZ = Z / Zhat - (1 - Z) / (1 - Zhat)\n",
    "    drZ = yhat1 - yhat0 + (y - yhat) * HZ\n",
    "    drD = Dhat1 - Dhat0 + (D - Dhat) * HZ\n",
    "    point = np.mean(drZ) / np.mean(drD)\n",
    "    psi = drZ - point * drD\n",
    "    Jhat = np.mean(drD)\n",
    "    var = np.mean(psi**2) / Jhat**2\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, Zhat, y - yhat, D - Dhat, Z - Zhat, drZ, drD\n",
    "\n",
    "def summary_iiv(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, drZ, drD, X, Z, D, y, *, name):\n",
    "    return pd.DataFrame({\n",
    "        'estimate': point,\n",
    "        'stderr': stderr,\n",
    "        'lower': point - 1.96 * stderr,\n",
    "        'upper': point + 1.96 * stderr,\n",
    "        'rmse y': np.sqrt(np.mean(resy**2)),\n",
    "        'rmse D': np.sqrt(np.mean(resD**2)),\n",
    "        'rmse Z': np.sqrt(np.mean(resZ**2)),\n",
    "        'accuracy D': np.mean(np.abs(resD) < 0.5),\n",
    "        'accuracy Z': np.mean(np.abs(resZ) < 0.5)\n",
    "    }, index=[name])\n",
    "\n",
    "def iivm_robust_inference(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, drZ, drD, X, Z, D, y, *, grid, alpha=0.05):\n",
    "    n = X.shape[0]\n",
    "    thr = scipy.stats.chi2.ppf(1 - alpha, df=1)\n",
    "    accept = []\n",
    "    for theta in grid:\n",
    "        moment = drZ - theta * drD\n",
    "        test = n * np.mean(moment)**2 / np.var(moment)\n",
    "        if test <= thr:\n",
    "            accept.append(theta)\n",
    "    return accept\n",
    "\n",
    "def run_iiv_analysis(data, desc, transformer):\n",
    "    print(f\"---------- IIV Analysis for {desc} ----------\")\n",
    "    y = data['net_tfa'].values\n",
    "    Z = data['e401'].values\n",
    "    D = data['p401'].values\n",
    "    X = data.drop(drop_cols, axis=1)\n",
    "    \n",
    "    results = []\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    # Method 1: Lasso + Logistic for IIV\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrd = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    lgrz = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    res = iiv(X, Z, D, y, lassoy, lassoy, lgrd, lgrd, lgrz, nfolds=3)\n",
    "    results.append(summary_iiv(*res, X, Z, D, y, name='lasso/logistic'))\n",
    "    \n",
    "    # Method 2: Random Forests for IIV\n",
    "    rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001))\n",
    "    rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001))\n",
    "    rfz = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001))\n",
    "    res = iiv(X, Z, D, y, rfy, rfy, rfd, rfd, rfz, nfolds=3)\n",
    "    results.append(summary_iiv(*res, X, Z, D, y, name='random forest'))\n",
    "    \n",
    "    # Method 3: Decision Trees for IIV\n",
    "    dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=0.001))\n",
    "    dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=0.001))\n",
    "    dtrz = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=0.001))\n",
    "    res = iiv(X, Z, D, y, dtry, dtry, dtrd, dtrd, dtrz, nfolds=3)\n",
    "    results.append(summary_iiv(*res, X, Z, D, y, name='decision trees'))\n",
    "    \n",
    "    # Method 4: Boosted Trees for IIV\n",
    "    gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "    gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    gbfz = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    res = iiv(X, Z, D, y, gbfy, gbfy, gbfd, gbfd, gbfz, nfolds=3)\n",
    "    results.append(summary_iiv(*res, X, Z, D, y, name='boosted trees'))\n",
    "    \n",
    "    # Method 5: AutoML for IIV via FLAML (using try/except)\n",
    "    try:\n",
    "        from flaml import AutoML\n",
    "        X_trans = transformer.fit_transform(X)\n",
    "        automl_reg = AutoML(time_budget=60, task='regression', early_stop=5, max_iter=100,\n",
    "                            eval_method='cv', n_splits=3, metric='r2', verbose=0)\n",
    "        automl_reg.fit(X_trans, y)\n",
    "        best_reg = automl_reg.best_estimator\n",
    "        automl_clf = AutoML(time_budget=60, task='classification', early_stop=5, max_iter=100,\n",
    "                            eval_method='cv', n_splits=3, metric='r2', verbose=0)\n",
    "        automl_clf.fit(X_trans, D)\n",
    "        best_d = automl_clf.best_estimator\n",
    "        automl_clf.fit(X_trans, Z)\n",
    "        best_z = automl_clf.best_estimator\n",
    "        modely0 = TransformedEstimator(transformer, clone(best_reg))\n",
    "        modely1 = TransformedEstimator(transformer, clone(best_reg))\n",
    "        modeld0 = TransformedEstimator(transformer, clone(best_d))\n",
    "        modeld1 = TransformedEstimator(transformer, clone(best_d))\n",
    "        modelz_auto = TransformedEstimator(transformer, clone(best_z))\n",
    "        res_auto = iiv(X, Z, D, y, modely0, modely1, modeld0, modeld1, modelz_auto, nfolds=3)\n",
    "        results.append(summary_iiv(*res_auto, X, Z, D, y, name='automl (semi-cfit)'))\n",
    "    except Exception as e:\n",
    "        print(\"FLAML AutoML for IIV failed, skipping AutoML method. Error:\", e)\n",
    "    \n",
    "    table_iiv = pd.concat(results)\n",
    "    print(table_iiv)\n",
    "    \n",
    "    region = iivm_robust_inference(*res, X, Z, D, y, grid=np.linspace(0, 20000, 10000))\n",
    "    print(f\"Robust IIV region: {np.min(region)} to {np.max(region)}\\n\")\n",
    "    \n",
    "    return table_iiv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis for Low Income (Bottom 25%) group...\n",
      "---------- PLIV Analysis for Low Income (Bottom 25%) ----------\n",
      "FLAML AutoML failed, skipping AutoML method. Error: `best_iteration` is only defined when early stopping is used.\n",
      "                   estimate       stderr        lower        upper  \\\n",
      "double lasso    5768.455193  1701.445143  2433.622713  9103.287673   \n",
      "lasso/logistic  5800.166817  1633.319043  2598.861493  9001.472141   \n",
      "random forest   6545.081883  1697.472956  3218.034889  9872.128876   \n",
      "decision tree   5794.913718  1664.321666  2532.843253  9056.984184   \n",
      "boosted forest  6455.054651  1692.226298  3138.291108  9771.818195   \n",
      "\n",
      "                      rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "double lasso    13446.444167  0.292899  0.342700    0.901130    0.847054  \n",
      "lasso/logistic  13446.444167  0.298584  0.358910    0.901130    0.847054  \n",
      "random forest   13482.012085  0.293862  0.344844    0.901130    0.846247  \n",
      "decision tree   14288.351637  0.310102  0.363336    0.876110    0.813156  \n",
      "boosted forest  13338.458365  0.293560  0.344026    0.900726    0.847054  \n",
      "Robust PLIV region: 3120.31203120312 to 9790.97909790979\n",
      "PLIV t-statistic (approx.): 976097.831\n",
      "\n",
      "---------- IIV Analysis for Low Income (Bottom 25%) ----------\n",
      "FLAML AutoML for IIV failed, skipping AutoML method. Error: `best_iteration` is only defined when early stopping is used.\n",
      "                   estimate       stderr        lower         upper  \\\n",
      "lasso/logistic  6326.624371  1604.203769  3182.384984   9470.863757   \n",
      "random forest   5936.627589  1736.287936  2533.503234   9339.751944   \n",
      "decision trees  1361.948223  4098.979605 -6672.051803   9395.948249   \n",
      "boosted trees   6861.156093  1603.480342  3718.334623  10003.977563   \n",
      "\n",
      "                      rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "lasso/logistic  13428.803538  0.188324  0.358910    0.945117    0.847054  \n",
      "random forest   13453.373215  0.189524  0.345161    0.947135    0.846247  \n",
      "decision trees  14817.024980  0.217951  0.364933    0.932607    0.810734  \n",
      "boosted trees   13504.254034  0.187404  0.343525    0.946731    0.847054  \n",
      "Robust IIV region: 3724.372437243724 to 10025.002500250024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Running analysis for Low Income (Bottom 25%) group...\")\n",
    "pliv_low = run_pliv_analysis(data_bottom, \"Low Income (Bottom 25%)\", transformer)\n",
    "iiv_low  = run_iiv_analysis(data_bottom, \"Low Income (Bottom 25%)\", transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis for High Income (Top 25%) group...\n",
      "---------- PLIV Analysis for High Income (Top 25%) ----------\n",
      "FLAML AutoML failed, skipping AutoML method. Error: `best_iteration` is only defined when early stopping is used.\n",
      "                    estimate       stderr         lower         upper  \\\n",
      "double lasso    23577.436393  5048.340429  13682.689152  33472.183634   \n",
      "lasso/logistic  23102.880506  5051.372643  13202.190124  33003.570887   \n",
      "random forest   22610.619713  5220.651521  12378.142733  32843.096694   \n",
      "decision tree   19271.997928  6855.090475   5836.020598  32707.975259   \n",
      "boosted forest  24267.441005  5296.199299  13886.890379  34647.991631   \n",
      "\n",
      "                       rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "double lasso     91149.933462  0.487619  0.483549    0.599435    0.591367  \n",
      "lasso/logistic   91149.933462  0.487782  0.482933    0.589351    0.600242  \n",
      "random forest    94743.172143  0.491518  0.484511    0.574425    0.608713  \n",
      "decision tree   100253.218084  0.556692  0.551192    0.526422    0.530859  \n",
      "boosted forest   99695.194928  0.491040  0.484648    0.565954    0.597015  \n",
      "Robust PLIV region: 13891.38913891389 to 20000.0\n",
      "PLIV t-statistic (approx.): 2159508.101\n",
      "\n",
      "---------- IIV Analysis for High Income (Top 25%) ----------\n",
      "FLAML AutoML for IIV failed, skipping AutoML method. Error: `best_iteration` is only defined when early stopping is used.\n",
      "                    estimate        stderr          lower         upper  \\\n",
      "lasso/logistic  22877.301868   5067.968315   12944.083971  32810.519764   \n",
      "random forest   22762.802664   5183.796590   12602.561349  32923.043980   \n",
      "decision trees -47312.130348  63151.826668 -171089.710617  76465.449922   \n",
      "boosted trees   24412.692351   5227.913744   14165.981413  34659.403289   \n",
      "\n",
      "                       rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "lasso/logistic   91267.974544  0.317979  0.482933    0.867689    0.600242  \n",
      "random forest    97158.793190  0.319450  0.484761    0.868495    0.605486  \n",
      "decision trees  102482.566021  0.371542  0.553612    0.797096    0.528439  \n",
      "boosted trees    98265.130713  0.316501  0.484377    0.868092    0.602662  \n",
      "Robust IIV region: 14167.416741674166 to 20000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Running analysis for High Income (Top 25%) group...\")\n",
    "pliv_high = run_pliv_analysis(data_top, \"High Income (Top 25%)\", transformer)\n",
    "iiv_high  = run_iiv_analysis(data_top, \"High Income (Top 25%)\", transformer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bottom 25%: We see high consistency in both PLV and IIV analysis, with one notable exception being decision tree under IIV, which output a value of ~1400 where others were clustered between 5000 and 6000.\n",
    "\n",
    "For top 25%: We see almost the exact same consistency at this income level as well, with the same exception for decision trees under IIV, which output a negative value whereas the rest were clustered between 22000 and 24000. \n",
    "\n",
    "Based on these results, we can in fact conclude that there exists heterogeneity with respect to 401k effect on income levels. At lower incomes, it seems to have a lower average affect on income, whereas this effect is much higher at the higher income levels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "\n",
    "The notebook only provides code for semi-crossfitting with automl. Using the pseudo code from the lecture notes, also implement semi-crossfitting with stacking (see also prior notebook on estimating effect of eligibility of 401k), where we pass a list of models for each input model parameter, then we fit and predict each of the models in a cross-fitting manner and using all the out of fold predictions we select a weighted combination based on OLS regression to combine all the predictions. We then use this weighted average of predictions as the nuisance predictions for each sample. After implementing this, report the same metrics as the metrics that the notebook reports when doing semi-crossfitting with automl and use the list of models as the ones that the notebook used unilaterally (e.g. for the outcome use a list of [lasso, random forest, gradient boosted forest] and for the treatment a list of [logistic regression, random forest classifier, gradient boosted classifier]). Do that for both the PLIV setting and the interactive IV setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Running PLIV Stacking Analysis ----------\n",
      "                    estimate      stderr        lower         upper  \\\n",
      "stacking (PLIV)  12925.98142  1875.43089  9250.136875  16601.825965   \n",
      "\n",
      "                       rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "stacking (PLIV)  53925.153227  0.414402  0.443224    0.745134    0.690066  \n",
      "Robust PLIV region: 9250.925092509251 to 16603.6603660366\n",
      "\n",
      "---------- Running IIV Stacking Analysis ----------\n",
      "                    estimate       stderr        lower         upper  \\\n",
      "stacking (IIV)  11180.221588  1637.570421  7970.583563  14389.859614   \n",
      "\n",
      "                      rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "stacking (IIV)  53698.051422  0.273537  0.443065    0.890973    0.692688  \n",
      "Robust IIV region: 7970.79707970797 to 14391.439143914391\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import (RandomForestRegressor, RandomForestClassifier, \n",
    "                              GradientBoostingRegressor, GradientBoostingClassifier)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "# Data loading and variable definitions \n",
    "\n",
    "file = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv\"\n",
    "data_all = pd.read_csv(file)\n",
    "data_all = data_all.dropna()\n",
    "\n",
    "# Define drop columns (as in your code)\n",
    "drop_cols = ['e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "             'hval', 'hmort', 'hequity',\n",
    "             'nifa', 'net_nifa', 'net_n401', 'ira',\n",
    "             'dum91', 'icat', 'ecat', 'zhat',\n",
    "             'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7',\n",
    "             'a1', 'a2', 'a3', 'a4', 'a5']\n",
    "\n",
    "# Define outcome, treatment, instrument and controls.\n",
    "y = data_all['net_tfa'].values\n",
    "Z = data_all['e401'].values\n",
    "D = data_all['p401'].values\n",
    "# Controls: drop the specified columns.\n",
    "X = data_all.drop(drop_cols, axis=1)\n",
    "\n",
    "\n",
    "#transformer and helper wrapper\n",
    "\n",
    "from formulaic import Formula\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FormulaTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, formula, array=False):\n",
    "        self.formula = formula\n",
    "        self.array = array\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        df = Formula(self.formula).get_model_matrix(X)\n",
    "        return df.values if self.array else df\n",
    "\n",
    "transformer = FormulaTransformer(\n",
    "    \"0 + poly(age, degree=6, raw=True) + poly(inc, degree=8, raw=True) \"\n",
    "    \"+ poly(educ, degree=4, raw=True) + poly(fsize, degree=2, raw=True) \"\n",
    "    \"+ male + marr + twoearn + db + pira + hown\", array=True)\n",
    "\n",
    "# A simple wrapper to combine a transformer with an estimator.\n",
    "class TransformedEstimator(BaseEstimator):\n",
    "    def __init__(self, transformer, estimator):\n",
    "        self.transformer = transformer\n",
    "        self.estimator = estimator\n",
    "    def fit(self, X, y):\n",
    "        X_trans = self.transformer.fit_transform(X)\n",
    "        self.estimator.fit(X_trans, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        X_trans = self.transformer.transform(X)\n",
    "        return self.estimator.predict(X_trans)\n",
    "    def predict_proba(self, X):\n",
    "        X_trans = self.transformer.transform(X)\n",
    "        return self.estimator.predict_proba(X_trans)\n",
    "\n",
    "\n",
    "# Summary and Inference functions\n",
    "\n",
    "def summary(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, epsilon, X, Z, D, y, *, name):\n",
    "    return pd.DataFrame({\n",
    "        'estimate': point,\n",
    "        'stderr': stderr,\n",
    "        'lower': point - 1.96 * stderr,\n",
    "        'upper': point + 1.96 * stderr,\n",
    "        'rmse y': np.sqrt(np.mean(resy**2)),\n",
    "        'rmse D': np.sqrt(np.mean(resD**2)),\n",
    "        'rmse Z': np.sqrt(np.mean(resZ**2)),\n",
    "        'accuracy D': np.mean(np.abs(resD) < 0.5),\n",
    "        'accuracy Z': np.mean(np.abs(resZ) < 0.5)\n",
    "    }, index=[name])\n",
    "\n",
    "def summary_iiv(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, drZ, drD, X, Z, D, y, *, name):\n",
    "    return pd.DataFrame({\n",
    "        'estimate': point,\n",
    "        'stderr': stderr,\n",
    "        'lower': point - 1.96 * stderr,\n",
    "        'upper': point + 1.96 * stderr,\n",
    "        'rmse y': np.sqrt(np.mean(resy**2)),\n",
    "        'rmse D': np.sqrt(np.mean(resD**2)),\n",
    "        'rmse Z': np.sqrt(np.mean(resZ**2)),\n",
    "        'accuracy D': np.mean(np.abs(resD) < 0.5),\n",
    "        'accuracy Z': np.mean(np.abs(resZ) < 0.5)\n",
    "    }, index=[name])\n",
    "\n",
    "def robust_inference(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, epsilon, X, Z, D, y, *, grid, alpha=0.05):\n",
    "    n = X.shape[0]\n",
    "    thr = scipy.stats.chi2.ppf(1 - alpha, df=1)\n",
    "    accept = []\n",
    "    for theta in grid:\n",
    "        moment = (resy - theta * resD) * resZ\n",
    "        test = n * np.mean(moment)**2 / np.var(moment)\n",
    "        if test <= thr:\n",
    "            accept.append(theta)\n",
    "    return accept\n",
    "\n",
    "def iivm_robust_inference(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, drZ, drD, X, Z, D, y, *, grid, alpha=0.05):\n",
    "    n = X.shape[0]\n",
    "    thr = scipy.stats.chi2.ppf(1 - alpha, df=1)\n",
    "    accept = []\n",
    "    for theta in grid:\n",
    "        moment = drZ - theta * drD\n",
    "        test = n * np.mean(moment)**2 / np.var(moment)\n",
    "        if test <= thr:\n",
    "            accept.append(theta)\n",
    "    return accept\n",
    "\n",
    "\n",
    "# Helper: Out‐of‐fold stacking prediction\n",
    "\n",
    "def stack_cv_prediction(model_list, X, y, cv, is_classifier=False):\n",
    "    \"\"\"\n",
    "    Compute out‐of‐fold predictions for each model in model_list and stack them via OLS.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_models = len(model_list)\n",
    "    base_preds = np.zeros((n_samples, n_models))\n",
    "    \n",
    "    for j, model in enumerate(model_list):\n",
    "        if is_classifier and hasattr(model, \"predict_proba\"):\n",
    "            preds = cross_val_predict(model, X, y, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "        else:\n",
    "            preds = cross_val_predict(model, X, y, cv=cv, n_jobs=-1)\n",
    "        base_preds[:, j] = preds\n",
    "\n",
    "    # Use OLS (LinearRegression) to stack the base predictions.\n",
    "    stacker = LinearRegression().fit(base_preds, y)\n",
    "    final_preds = stacker.predict(base_preds)\n",
    "    return final_preds, base_preds, stacker\n",
    "\n",
    "\n",
    "# 1) PLIV stacking function\n",
    "\n",
    "def dml_pliv_stacking(\n",
    "    X, Z, D, y,\n",
    "    modely_list,    # list of models for outcome Y\n",
    "    modeld_list,    # list of models for treatment D\n",
    "    modelz_list,    # list of models for instrument Z\n",
    "    nfolds=3,\n",
    "    classifier_d=False,\n",
    "    classifier_z=False,\n",
    "    random_state=123\n",
    "):\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Outcome stacking\n",
    "    yhat, _, _ = stack_cv_prediction(modely_list, X, y, cv, is_classifier=False)\n",
    "    \n",
    "    # Treatment stacking\n",
    "    Dhat, _, _ = stack_cv_prediction(modeld_list, X, D, cv, is_classifier=classifier_d)\n",
    "    \n",
    "    # Instrument stacking\n",
    "    Zhat, _, _ = stack_cv_prediction(modelz_list, X, Z, cv, is_classifier=classifier_z)\n",
    "    \n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "    resZ = Z - Zhat\n",
    "    point = np.mean(resy * resZ) / np.mean(resD * resZ)\n",
    "    epsilon = resy - point * resD\n",
    "    var = np.mean(epsilon**2 * resZ**2) / (np.mean(resD * resZ)**2)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    \n",
    "    return point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, epsilon\n",
    "\n",
    "# 2) IIV stacking function\n",
    "def iiv_stacking(\n",
    "    X, Z, D, y,\n",
    "    modely0_list, modely1_list,   # models for outcome in Z=0 and Z=1 groups\n",
    "    modeld0_list, modeld1_list,   # models for treatment in Z=0 and Z=1 groups\n",
    "    modelz_list,                  # models for instrument Z\n",
    "    trimming=0.01,\n",
    "    nfolds=3,\n",
    "    random_state=123\n",
    "):\n",
    "    n = X.shape[0]\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Initialize storage for out‐of‐fold predictions\n",
    "    y0_preds = np.zeros((n, len(modely0_list)))\n",
    "    y1_preds = np.zeros((n, len(modely1_list)))\n",
    "    d0_preds = np.zeros((n, len(modeld0_list)))\n",
    "    d1_preds = np.zeros((n, len(modeld1_list)))\n",
    "    \n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        Z_train = Z[train_idx]\n",
    "        D_train = D[train_idx]\n",
    "        \n",
    "        # Outcome models for Z=0\n",
    "        idx_z0 = (Z_train == 0)\n",
    "        if np.sum(idx_z0) > 0:\n",
    "            for j, model in enumerate(modely0_list):\n",
    "                mdl = clone(model).fit(X_train[idx_z0], y_train[idx_z0])\n",
    "                y0_preds[test_idx, j] = mdl.predict(X_test)\n",
    "                \n",
    "        # Outcome models for Z=1\n",
    "        idx_z1 = (Z_train == 1)\n",
    "        if np.sum(idx_z1) > 0:\n",
    "            for j, model in enumerate(modely1_list):\n",
    "                mdl = clone(model).fit(X_train[idx_z1], y_train[idx_z1])\n",
    "                y1_preds[test_idx, j] = mdl.predict(X_test)\n",
    "                \n",
    "        # Treatment models for Z=0\n",
    "        D_train_z0 = D_train[Z_train == 0]\n",
    "        if np.std(D_train_z0) < 1e-10:\n",
    "            d0_preds[test_idx, :] = np.mean(D_train_z0)\n",
    "        else:\n",
    "            for j, model in enumerate(modeld0_list):\n",
    "                mdl = clone(model).fit(X_train[Z_train==0], D_train_z0)\n",
    "                if hasattr(mdl, \"predict_proba\"):\n",
    "                    d0_preds[test_idx, j] = mdl.predict_proba(X_test)[:,1]\n",
    "                else:\n",
    "                    d0_preds[test_idx, j] = mdl.predict(X_test)\n",
    "                    \n",
    "        # Treatment models for Z=1\n",
    "        D_train_z1 = D_train[Z_train == 1]\n",
    "        if np.std(D_train_z1) < 1e-10:\n",
    "            d1_preds[test_idx, :] = np.mean(D_train_z1)\n",
    "        else:\n",
    "            for j, model in enumerate(modeld1_list):\n",
    "                mdl = clone(model).fit(X_train[Z_train==1], D_train_z1)\n",
    "                if hasattr(mdl, \"predict_proba\"):\n",
    "                    d1_preds[test_idx, j] = mdl.predict_proba(X_test)[:,1]\n",
    "                else:\n",
    "                    d1_preds[test_idx, j] = mdl.predict(X_test)\n",
    "    \n",
    "    # Stack predictions on each subgroup via OLS.\n",
    "    idx_all = np.arange(n)\n",
    "    idx_z0_all = (Z == 0)\n",
    "    idx_z1_all = (Z == 1)\n",
    "    \n",
    "    # Stack outcomes\n",
    "    stacker_y0 = LinearRegression().fit(y0_preds[idx_z0_all], y[idx_z0_all])\n",
    "    final_y0 = stacker_y0.predict(y0_preds)\n",
    "    stacker_y1 = LinearRegression().fit(y1_preds[idx_z1_all], y[idx_z1_all])\n",
    "    final_y1 = stacker_y1.predict(y1_preds)\n",
    "    \n",
    "    # Stack treatments\n",
    "    if np.var(D[idx_z0_all]) > 1e-10:\n",
    "        stacker_d0 = LinearRegression().fit(d0_preds[idx_z0_all], D[idx_z0_all])\n",
    "        final_d0 = stacker_d0.predict(d0_preds)\n",
    "    else:\n",
    "        final_d0 = np.full(n, np.mean(D[idx_z0_all]))\n",
    "    if np.var(D[idx_z1_all]) > 1e-10:\n",
    "        stacker_d1 = LinearRegression().fit(d1_preds[idx_z1_all], D[idx_z1_all])\n",
    "        final_d1 = stacker_d1.predict(d1_preds)\n",
    "    else:\n",
    "        final_d1 = np.full(n, np.mean(D[idx_z1_all]))\n",
    "        \n",
    "    # Combine according to observed Z.\n",
    "    yhat = final_y0 * (1 - Z) + final_y1 * Z\n",
    "    Dhat = final_d0 * (1 - Z) + final_d1 * Z\n",
    "    \n",
    "    # Instrument stacking via standard CV stacking.\n",
    "    cv_z = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    z_base_preds = np.zeros((n, len(modelz_list)))\n",
    "    for j, model in enumerate(modelz_list):\n",
    "        if set(np.unique(Z)) == {0,1} and hasattr(model, \"predict_proba\"):\n",
    "            preds = cross_val_predict(model, X, Z, cv=cv_z, method='predict_proba', n_jobs=-1)[:,1]\n",
    "        else:\n",
    "            preds = cross_val_predict(model, X, Z, cv=cv_z, n_jobs=-1)\n",
    "        z_base_preds[:, j] = preds\n",
    "    stacker_z = LinearRegression().fit(z_base_preds, Z)\n",
    "    Zhat = stacker_z.predict(z_base_preds)\n",
    "    Zhat = np.clip(Zhat, trimming, 1 - trimming)\n",
    "    \n",
    "    # Doubly robust transformation for IIV.\n",
    "    HZ = (Z / Zhat) - ((1 - Z) / (1 - Zhat))\n",
    "    drZ = final_y1 - final_y0 + (y - yhat) * HZ\n",
    "    drD = final_d1 - final_d0 + (D - Dhat) * HZ\n",
    "    point = np.mean(drZ) / np.mean(drD)\n",
    "    psi = drZ - point * drD\n",
    "    Jhat = np.mean(drD)\n",
    "    var = np.mean(psi**2) / (Jhat**2)\n",
    "    stderr = np.sqrt(var / n)\n",
    "    \n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "    resZ = Z - Zhat\n",
    "    \n",
    "    return point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, drZ, drD\n",
    "\n",
    "\n",
    "# Fallback functions (use only the first model from each list)\n",
    "\n",
    "def dml_pliv_fallback(X, Z, D, y, modely, modeld, modelz, nfolds=3, classifier_d=False, classifier_z=False, random_state=123):\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    yhat = cross_val_predict(modely, X, y, cv=cv, n_jobs=-1)\n",
    "    if classifier_d and hasattr(modeld, \"predict_proba\"):\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:,1]\n",
    "    else:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1)\n",
    "    if classifier_z and hasattr(modelz, \"predict_proba\"):\n",
    "        Zhat = cross_val_predict(modelz, X, Z, cv=cv, method='predict_proba', n_jobs=-1)[:,1]\n",
    "    else:\n",
    "        Zhat = cross_val_predict(modelz, X, Z, cv=cv, n_jobs=-1)\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "    resZ = Z - Zhat\n",
    "    point = np.mean(resy * resZ) / np.mean(resD * resZ)\n",
    "    epsilon = resy - point * resD\n",
    "    var = np.mean(epsilon**2 * resZ**2) / (np.mean(resD * resZ)**2)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, epsilon\n",
    "\n",
    "def iiv_fallback(X, Z, D, y, modely0, modely1, modeld0, modeld1, modelz, nfolds=3, trimming=0.01, random_state=123):\n",
    "    n = X.shape[0]\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    y0_preds = np.zeros(n)\n",
    "    y1_preds = np.zeros(n)\n",
    "    d0_preds = np.zeros(n)\n",
    "    d1_preds = np.zeros(n)\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        Z_train = Z[train_idx]\n",
    "        D_train = D[train_idx]\n",
    "        if np.any(Z_train==0):\n",
    "            mdl_y0 = clone(modely0).fit(X_train[Z_train==0], y_train[Z_train==0])\n",
    "            y0_preds[test_idx] = mdl_y0.predict(X_test)\n",
    "            mdl_d0 = clone(modeld0).fit(X_train[Z_train==0], D_train[Z_train==0])\n",
    "            if hasattr(mdl_d0, \"predict_proba\"):\n",
    "                d0_preds[test_idx] = mdl_d0.predict_proba(X_test)[:,1]\n",
    "            else:\n",
    "                d0_preds[test_idx] = mdl_d0.predict(X_test)\n",
    "        if np.any(Z_train==1):\n",
    "            mdl_y1 = clone(modely1).fit(X_train[Z_train==1], y_train[Z_train==1])\n",
    "            y1_preds[test_idx] = mdl_y1.predict(X_test)\n",
    "            mdl_d1 = clone(modeld1).fit(X_train[Z_train==1], D_train[Z_train==1])\n",
    "            if hasattr(mdl_d1, \"predict_proba\"):\n",
    "                d1_preds[test_idx] = mdl_d1.predict_proba(X_test)[:,1]\n",
    "            else:\n",
    "                d1_preds[test_idx] = mdl_d1.predict(X_test)\n",
    "    yhat = y0_preds * (1 - Z) + y1_preds * Z\n",
    "    Dhat = d0_preds * (1 - Z) + d1_preds * Z\n",
    "    cv2 = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    if set(np.unique(Z)) == {0,1} and hasattr(modelz, \"predict_proba\"):\n",
    "        Zhat = cross_val_predict(modelz, X, Z, cv=cv2, method='predict_proba', n_jobs=-1)[:,1]\n",
    "    else:\n",
    "        Zhat = cross_val_predict(modelz, X, Z, cv=cv2, n_jobs=-1)\n",
    "    Zhat = np.clip(Zhat, trimming, 1-trimming)\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "    resZ = Z - Zhat\n",
    "    HZ = (Z / Zhat) - ((1-Z)/(1-Zhat))\n",
    "    drZ = y1_preds - y0_preds + (y - yhat) * HZ\n",
    "    drD = d1_preds - d0_preds + (D - Dhat) * HZ\n",
    "    point = np.mean(drZ) / np.mean(drD)\n",
    "    psi = drZ - point * drD\n",
    "    Jhat = np.mean(drD)\n",
    "    var = np.mean(psi**2) / (Jhat**2)\n",
    "    stderr = np.sqrt(var / n)\n",
    "    return point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, drZ, drD\n",
    "\n",
    "\n",
    "# Define base learners \n",
    "\n",
    "# Outcome models\n",
    "lasso_y = make_pipeline(transformer, StandardScaler(), LassoCV(cv=5))\n",
    "rf_y    = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001))\n",
    "gbf_y   = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "modely_list = [lasso_y, rf_y, gbf_y]\n",
    "\n",
    "# Treatment models (binary, so use classifiers)\n",
    "lgr_d = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=5))\n",
    "rf_d  = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001))\n",
    "gbf_d = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "modeld_list = [lgr_d, rf_d, gbf_d]\n",
    "\n",
    "# Instrument models (binary)\n",
    "lgr_z = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=5))\n",
    "rf_z  = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=0.001))\n",
    "gbf_z = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "modelz_list = [lgr_z, rf_z, gbf_z]\n",
    "\n",
    "# For IIV, we use clones for Z=0 and Z=1 separately.\n",
    "modely0_list = [clone(m) for m in modely_list]\n",
    "modely1_list = [clone(m) for m in modely_list]\n",
    "modeld0_list = [clone(m) for m in modeld_list]\n",
    "modeld1_list = [clone(m) for m in modeld_list]\n",
    "modelz_iiv_list = [clone(m) for m in modelz_list]\n",
    "\n",
    "\n",
    "# Main Execution with try/except fallback for PLIV and IIV stacking analyses\n",
    "\n",
    "grid = np.linspace(0, 20000, 10000)\n",
    "\n",
    "print(\"---------- Running PLIV Stacking Analysis ----------\")\n",
    "try:\n",
    "    pliv_res = dml_pliv_stacking(X, Z, D, y,\n",
    "                                 modely_list=modely_list,\n",
    "                                 modeld_list=modeld_list,\n",
    "                                 modelz_list=modelz_list,\n",
    "                                 nfolds=3,\n",
    "                                 classifier_d=True,\n",
    "                                 classifier_z=True)\n",
    "    table_stack = summary(*pliv_res, X, Z, D, y, name='stacking (PLIV)')\n",
    "    print(table_stack)\n",
    "    \n",
    "    region = robust_inference(*pliv_res, X, Z, D, y, grid=grid)\n",
    "    print(f\"Robust PLIV region: {np.min(region)} to {np.max(region)}\")\n",
    "except Exception as e:\n",
    "    print(\"Error in PLIV stacking:\", e)\n",
    "    print(\"Running fallback PLIV using first models only.\")\n",
    "    pliv_res = dml_pliv_fallback(X, Z, D, y,\n",
    "                                 modely=modely_list[0],\n",
    "                                 modeld=modeld_list[0],\n",
    "                                 modelz=modelz_list[0],\n",
    "                                 nfolds=3,\n",
    "                                 classifier_d=True,\n",
    "                                 classifier_z=True)\n",
    "    table_stack = summary(*pliv_res, X, Z, D, y, name='fallback (PLIV)')\n",
    "    print(table_stack)\n",
    "    region = robust_inference(*pliv_res, X, Z, D, y, grid=grid)\n",
    "    print(f\"Robust PLIV region (fallback): {np.min(region)} to {np.max(region)}\")\n",
    "    \n",
    "print(\"\\n---------- Running IIV Stacking Analysis ----------\")\n",
    "try:\n",
    "    iiv_res = iiv_stacking(X, Z, D, y,\n",
    "                           modely0_list, modely1_list,\n",
    "                           modeld0_list, modeld1_list,\n",
    "                           modelz_iiv_list,\n",
    "                           trimming=0.01,\n",
    "                           nfolds=3)\n",
    "    table_iiv = summary_iiv(*iiv_res, X, Z, D, y, name='stacking (IIV)')\n",
    "    print(table_iiv)\n",
    "    \n",
    "    region_iiv = iivm_robust_inference(*iiv_res, X, Z, D, y, grid=grid)\n",
    "    print(f\"Robust IIV region: {np.min(region_iiv)} to {np.max(region_iiv)}\")\n",
    "except Exception as e:\n",
    "    print(\"Error in IIV stacking:\", e)\n",
    "    print(\"Running fallback IIV using first models only.\")\n",
    "    iiv_res = iiv_fallback(X, Z, D, y,\n",
    "                           modely0=modely0_list[0],\n",
    "                           modely1=modely1_list[0],\n",
    "                           modeld0=modeld0_list[0],\n",
    "                           modeld1=modeld1_list[0],\n",
    "                           modelz=modelz_iiv_list[0],\n",
    "                           nfolds=3,\n",
    "                           trimming=0.01)\n",
    "    table_iiv = summary_iiv(*iiv_res, X, Z, D, y, name='fallback (IIV)')\n",
    "    print(table_iiv)\n",
    "    region_iiv = iivm_robust_inference(*iiv_res, X, Z, D, y, grid=grid)\n",
    "    print(f\"Robust IIV region (fallback): {np.min(region_iiv)} to {np.max(region_iiv)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We begin with the two moment conditions:\n",
    "  \n",
    "$$\n",
    "\\mathbb{E}\\Big[\\Big(\\tilde{Y} - \\alpha\\,\\tilde{D} - \\delta'\\,\\tilde{S}\\Big)\\,\\tilde{D}\\Big] = 0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\Big[\\Big(\\tilde{Y} - \\alpha\\,\\tilde{D} - \\delta'\\,\\tilde{S}\\Big)\\,\\tilde{Q}\\Big] = 0.\n",
    "$$\n",
    "\n",
    "These conditions can be compactly written as a vector of moment equations:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\Big[m(W; \\theta, \\eta)\\Big] = 0,\n",
    "$$\n",
    "\n",
    "where the parameter vector is\n",
    "\n",
    "$$\n",
    "\\theta = (\\alpha,\\, \\delta),\n",
    "$$\n",
    "\n",
    "and the moment function is defined as\n",
    "\n",
    "$$\n",
    "m(W; \\theta, \\eta) = \\begin{pmatrix}\n",
    "\\Big(\\tilde{Y} - \\alpha\\,\\tilde{D} - \\delta'\\,\\tilde{S}\\Big)\\,\\tilde{D} \\\\\n",
    "\\Big(\\tilde{Y} - \\alpha\\,\\tilde{D} - \\delta'\\,\\tilde{S}\\Big)\\,\\tilde{Q}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Here, the residualized variables are obtained by “partialling out” the effect of controls $X$:\n",
    "  \n",
    "$$\n",
    "\\tilde{Y} = Y - \\mathbb{E}[Y|X], \\quad \\tilde{D} = D - \\mathbb{E}[D|X], \\quad \\tilde{S} = S - \\mathbb{E}[S|X], \\quad \\tilde{Q} = Q - \\mathbb{E}[Q|X].\n",
    "$$\n",
    "\n",
    "The collection of nuisance functions, denoted by $\\eta$, includes the conditional expectation functions (used to form the residuals) and also the projection (or “best linear prediction”) step. In particular, when the technical instrument dimension exceeds that of the technical treatment, we estimate the projection matrix $B$ from regressing $\\tilde{D}$ on $\\tilde{Q}$:\n",
    "  \n",
    "$$\n",
    "B = \\mathbb{E}\\Big[\\tilde{V}\\,\\tilde{V}'\\Big]^{-1} \\mathbb{E}\\Big[\\tilde{V}\\,\\tilde{D}\\Big],\n",
    "$$\n",
    "\n",
    "with $\\tilde{V} = (\\tilde{D},\\, \\tilde{Q})$. This projection is used to form the “new” instrument that enters our moment conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### Framing as a Neyman-Orthogonal Moment Condition\n",
    "\n",
    "The key idea is to express the estimation of $\\theta = (\\alpha, \\delta)$ via the moment function $m(W; \\theta, \\eta)$ that depends on additional nuisance parameters $\\eta$. Our estimator solves the empirical analogue\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^n m(W_i; \\theta, \\eta) = 0,\n",
    "$$\n",
    "\n",
    "with $\\eta$ estimated separately (using, for example, cross-fitting).\n",
    "\n",
    "**Neyman Orthogonality** requires that the moment function be locally insensitive to small errors in the estimation of $\\eta$. Formally, if we perturb the nuisance functions by a small amount $h$, the Gateaux derivative at the true nuisance parameter $\\eta_0$ should satisfy\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial t}\\Bigg|_{t=0} \\mathbb{E}\\Big[m(W; \\theta_0, \\eta_0 + t\\, h)\\Big] = 0.\n",
    "$$\n",
    "\n",
    "In our setup this orthogonality is achieved because:\n",
    "\n",
    "1. **Residualization:**  \n",
    "   The construction of $\\tilde{Y}$, $\\tilde{D}$, $\\tilde{S}$, and $\\tilde{Q}$ via regressing out the controls ensures that any estimation error in $\\mathbb{E}[\\,\\cdot\\,|X]$ is orthogonal to the residuals. In other words, the residuals are by construction uncorrelated with the nuisance estimates.\n",
    "\n",
    "2. **Projection Step:**  \n",
    "   The use of the projection matrix $B$ in forming the technical instruments further “purges” the variation that could be contaminated by errors in the nuisance estimates. Because $B$ is obtained by an OLS projection, its estimation error has a first‐order negligible effect on the moment conditions.\n",
    "\n",
    "3. **Moment Structure:**  \n",
    "   The moment function is linear in the residualized variables. Thus, any small error in the nuisance components (which enter additively when forming $\\tilde{Y}$, $\\tilde{D}$, etc.) does not change the first-order behavior of the moment condition. This guarantees that the derivative of the expectation of $m(W; \\theta, \\eta)$ with respect to $\\eta$ is zero at $\\eta_0$.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "By casting the estimation of $(\\alpha, \\delta)$ into the framework\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\Big[\\begin{pmatrix}\n",
    "\\Big(\\tilde{Y} - \\alpha\\,\\tilde{D} - \\delta'\\,\\tilde{S}\\Big)\\,\\tilde{D} \\\\\n",
    "\\Big(\\tilde{Y} - \\alpha\\,\\tilde{D} - \\delta'\\,\\tilde{S}\\Big)\\,\\tilde{Q}\n",
    "\\end{pmatrix}\\Big] = 0,\n",
    "$$\n",
    "\n",
    "we are in the standard setting of moment-based estimation with nuisance parameters. The residualization and projection steps ensure that this vector of moment equations satisfies the Neyman orthogonality property, making our estimator robust to first-order errors in the estimation of the nuisance functions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the parameter vector be \n",
    "$$\n",
    "\\theta = (\\alpha,\\, \\delta),\n",
    "$$ \n",
    "and consider the moment condition\n",
    "$$\n",
    "\\mathbb{E}\\Big[\\Big(\\tilde{Y} - \\alpha\\,\\tilde{D} - \\delta'\\,\\tilde{S}\\Big)\\,\\tilde{Z}\\Big] = 0,\n",
    "$$\n",
    "where the instrument vector is\n",
    "$$\n",
    "\\tilde{Z} = (\\tilde{D},\\, \\tilde{Q})\n",
    "$$\n",
    "and the regressor vector is\n",
    "$$\n",
    "\\tilde{W} = (\\tilde{D},\\, \\tilde{S}).\n",
    "$$\n",
    "\n",
    "Under the general theorem for parameters defined via Neyman orthogonal linear moment restrictions, the estimator $\\hat{\\theta}$ that solves the empirical moment equations is asymptotically normal:\n",
    "$$\n",
    "\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\overset{d}{\\to} N(0, V),\n",
    "$$\n",
    "with asymptotic covariance matrix\n",
    "$$\n",
    "V = A^{-1} B (A^{-1})',\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "A = \\mathbb{E}\\left[\\frac{\\partial m(W; \\theta, \\eta)}{\\partial \\theta}\\right],\n",
    "$$ \n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "B = \\mathbb{E}\\Big[m(W; \\theta, \\eta)m(W; \\theta, \\eta)'\\Big].\n",
    "$$\n",
    "\n",
    "A numerically equivalent estimation algorithm uses a two-stage least squares (2SLS) approach on the residualized system. The procedure is as follows:\n",
    "\n",
    "1. **Residualization:**  \n",
    "   For each observation $i$, estimate the nuisance functions using cross-fitting and obtain the residuals:\n",
    "   $$\n",
    "   \\tilde{Y}_i = Y_i - \\widehat{\\mathbb{E}}[Y|X_i],\\quad \\tilde{D}_i = D_i - \\widehat{\\mathbb{E}}[D|X_i],\n",
    "   $$\n",
    "   $$\n",
    "   \\tilde{S}_i = S_i - \\widehat{\\mathbb{E}}[S|X_i],\\quad \\tilde{Q}_i = Q_i - \\widehat{\\mathbb{E}}[Q|X_i].\n",
    "   $$\n",
    "\n",
    "2. **First-Stage Projection:**  \n",
    "   Regress $\\tilde{D}$ on $\\tilde{Q}$ via OLS (fitting the low-dimensional projection matrix on all the data) to obtain the fitted values\n",
    "   $$\n",
    "   \\hat{D}_i = B\\,\\tilde{Q}_i.\n",
    "   $$\n",
    "   Form the instrument vector for the second stage as\n",
    "   $$\n",
    "   \\hat{Z}_i = (\\hat{D}_i,\\, \\tilde{Q}_i).\n",
    "   $$\n",
    "\n",
    "3. **Second-Stage Regression (2SLS):**  \n",
    "   Run OLS of $\\tilde{Y}$ on $\\tilde{D}$ and $\\tilde{S}$ using the instruments $\\hat{Z}_i$ to obtain the estimator \n",
    "   $$\n",
    "   \\hat{\\theta} = (\\hat{\\alpha},\\, \\hat{\\delta}).\n",
    "   $$\n",
    "\n",
    "4. **Residual Calculation:**  \n",
    "   Compute the second-stage residuals:\n",
    "   $$\n",
    "   \\hat{u}_i = \\tilde{Y}_i - \\hat{\\alpha}\\,\\tilde{D}_i - \\hat{\\delta}'\\,\\tilde{S}_i.\n",
    "   $$\n",
    "\n",
    "5. **Asymptotic Variance Estimation:**  \n",
    "   The asymptotic covariance matrix for $\\hat{\\theta}$ is estimated by the standard 2SLS formula:\n",
    "   $$\n",
    "   \\widehat{V} = \\left(\\frac{1}{n}\\sum_{i=1}^n \\hat{Z}_i\\,\\tilde{W}_i'\\right)^{-1} \\left(\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i^2\\, \\hat{Z}_i\\,\\hat{Z}_i'\\right) \\left(\\frac{1}{n}\\sum_{i=1}^n \\hat{Z}_i\\,\\tilde{W}_i'\\right)^{-1\\prime},\n",
    "   $$\n",
    "   where\n",
    "   $$\n",
    "   \\tilde{W}_i = (\\tilde{D}_i,\\, \\tilde{S}_i).\n",
    "   $$\n",
    "\n",
    "6. **Standard Error for $\\hat{\\alpha}$:**  \n",
    "   The asymptotic variance of $\\hat{\\alpha}$ is the $(1,1)$ element of $\\widehat{V}$:\n",
    "   $$\n",
    "   \\text{Var}(\\hat{\\alpha}) \\approx \\widehat{V}_{11},\n",
    "   $$\n",
    "   so the standard error is\n",
    "   $$\n",
    "   \\text{SE}(\\hat{\\alpha}) = \\sqrt{\\widehat{V}_{11}}.\n",
    "   $$\n",
    "\n",
    "**Algorithm Summary:**\n",
    "\n",
    "- **Step 1:** Residualize $Y$, $D$, $S$, and $Q$ by regressing each on $X$ (using cross-fitting) to obtain $\\tilde{Y}$, $\\tilde{D}$, $\\tilde{S}$, and $\\tilde{Q}$.\n",
    "- **Step 2:** Regress $\\tilde{D}$ on $\\tilde{Q}$ via OLS to obtain $B$ and fitted values $\\hat{D}$; construct $\\hat{Z} = (\\hat{D}, \\tilde{Q})$.\n",
    "- **Step 3:** Run 2SLS: regress $\\tilde{Y}$ on $(\\tilde{D}, \\tilde{S})$ using instruments $\\hat{Z}$ to obtain $\\hat{\\theta} = (\\hat{\\alpha}, \\hat{\\delta})$.\n",
    "- **Step 4:** Compute residuals $\\hat{u}_i = \\tilde{Y}_i - \\hat{\\alpha}\\,\\tilde{D}_i - \\hat{\\delta}'\\,\\tilde{S}_i$.\n",
    "- **Step 5:** Estimate\n",
    "  $$\n",
    "  \\widehat{V} = \\left(\\frac{1}{n}\\sum_{i=1}^n \\hat{Z}_i\\,\\tilde{W}_i'\\right)^{-1} \\left(\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i^2\\, \\hat{Z}_i\\,\\hat{Z}_i'\\right) \\left(\\frac{1}{n}\\sum_{i=1}^n \\hat{Z}_i\\,\\tilde{W}_i'\\right)^{-1\\prime}.\n",
    "  $$\n",
    "- **Step 6:** The standard error for $\\hat{\\alpha}$ is given by\n",
    "  $$\n",
    "  \\text{SE}(\\hat{\\alpha}) = \\sqrt{\\widehat{V}_{11}}.\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALL_PYTHON",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
